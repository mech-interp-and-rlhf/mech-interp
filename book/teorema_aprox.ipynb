{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Teorema de Aproximación Universal "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las redes neuronales, independientemente de la tarea que estén resolviendo, pueden concebirse matemáticamente como funciones parametrizadas. A través del entrenamiento, ajustan sus parámetros para aproximar una función deseada que relaciona ciertos datos de entrada con una salida esperada. Esta perspectiva unificadora es válida para tareas tan diversas como clasificación, reconstrucción, predicción o generación, y está respaldada por el Teorema de Aproximación Universal, el cual garantiza que, bajo ciertas condiciones, una red puede aproximar cualquier función continua con precisión arbitraria.\n",
    "\n",
    "Por ejemplo, en la clasificación de objetos, la red aprende a mapear una entrada (como una imagen) hacia una categoría específica. Desde el punto de vista funcional, esto equivale a aproximar una función que asocia cada vector de entrada con un vector de probabilidades, cuya componente mayor indica la clase predicha. Aunque el objetivo final es una clase discreta, lo que la red realmente está aprendiendo es una función continua que asigna probabilidades a cada clase posible.\n",
    "\n",
    "En tareas como los autoencoders, la red tiene como objetivo reconstruir su propia entrada. Esto se formula como una función identidad aproximada: se busca que $\n",
    "f(x) \\approx x\n",
    "$ pero con la condición de que la información pase por un espacio comprimido o latente. Aquí también, la red está aprendiendo una función continua que intenta replicar sus entradas lo más fielmente posible, a pesar de haberlas codificado en menos dimensiones intermedias.\n",
    "\n",
    "En el caso de eliminación de ruido (denoising), se trata de recuperar una versión \"limpia\" de una entrada corrupta. En términos funcionales, esto es una aproximación de la forma $\n",
    "f(\\tilde{x}) \\approx x\n",
    "$ donde $\\tilde{x}$ es una versión con ruido de $x$. La red aprende una función que transforma una entrada perturbada en una salida suavizada y coherente, lo cual, nuevamente, se modela como una función continua.\n",
    "\n",
    "En modelos de generación de texto (como los basados en transformers), lo que la red hace es predecir la siguiente palabra o token dado un contexto anterior. Esto puede verse como la aproximación de una función que toma una secuencia y devuelve una distribución de probabilidad sobre el vocabulario, una función no lineal y dependiente del contexto, pero continua en su construcción interna.\n",
    "\n",
    "En todos estos ejemplos, la red, aunque esté compuesta por múltiples capas, funciones de activación y parámetros, define al final una única función compuesta, continua en la mayoría de los casos, que aproxima la relación deseada entre entrada y salida. Y gracias a los resultados del teorema de aproximación universal, sabemos que, si el diseño es adecuado y hay suficientes recursos (como número de neuronas o datos), esta aproximación puede ser tan buena como se necesite.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Teorema de Aproximación Universal\n",
    "\n",
    "Sea $\\sigma$ una función continua y discriminatoria. Entonces, las sumas finitas de la forma:\n",
    "\n",
    "$$\n",
    "G(x) = \\sum_{j=1}^N \\alpha_j \\, \\sigma(w_j^T x + b_j), \\quad \\text{donde } w_j \\in \\mathbb{R}^n, \\; \\alpha_j, b_j \\in \\mathbb{R}\n",
    "$$\n",
    "\n",
    "son densas en el espacio $C(I_n)$.\n",
    "\n",
    "Es decir, dada cualquier función continua $f \\in C(I_n)$ y cualquier $\\varepsilon > 0$, existe una suma $G(x)$ de la forma anterior tal que:\n",
    "\n",
    "$$\n",
    "|G(x) - f(x)| < \\varepsilon \\quad \\text{para todo } x \\in I_n\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "<details>\n",
    "<summary> Nota: Sobre el Teorema </summary>\n",
    "\n",
    "Hay varias formulaciones del Teorema de aproximacion universal. La primera demostración se le atribuye al matemático Cybenko en 1989. Las\n",
    "distintas formalaciomes que hay sobre este teorema tienen que ver con distintos \n",
    "tipos de restricciones en la funcion de activación. \n",
    "La formulacion que presetamos corresponde al siguiente documento: \n",
    "[https://users.wpi.edu/~msarkis/BrownBag/Elisa1.pdf](URL) \n",
    "La razon de utilizarla es porque es bastante facil de leer y segur la demostracion que se presenta. \n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una versión informal de este teorema equivale a decir que una red feedforward \n",
    "con tres capas, que utilice una función de activación no lineal adecuada, \n",
    "puede aproximar cualquier función continua\n",
    "sobre un intervalo definido de $\\mathbb{R}^n$, con suficiente número de\n",
    "neuronas.\n",
    "\n",
    "Podemos notar que realmente existe una comparación entre el teorema y esta versión informal, ya que si observamos esta parte:\n",
    "$\\alpha_j ,\\sigma(w_j^T x + b_j)$\n",
    "podemos deducir que esa combinación deja explícito que debe haber por lo menos tres capas. ¿Cuáles son estas capas? \n",
    "\n",
    "Primero tendríamos la de entrada, que corresponde a $x$; después, una capa oculta que es el resultado de una combinación afín con una función de activación; y, posteriormente, una capa más donde solo se está haciendo una combinación lineal, que bien podríamos considerar como el caso de la función afín en el caso donde el término independiente es igual a cero."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
