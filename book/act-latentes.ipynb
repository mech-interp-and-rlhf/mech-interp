{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpretación de latentes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch\n",
    "from torch import nn\n",
    "import jaxtyping\n",
    "import datasets\n",
    "from datasets import load_dataset\n",
    "from sae import Step\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MINI_BATCH_SIZE = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint cached at -> /home/admin/.cache/huggingface/hub/models--mech-interp-uam--llama3.2-1b-sae/snapshots/891afeb81f0a19d6a791b6b8a3110bc3d87c3f5f/sae.pth\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "ckpt_path = hf_hub_download(\n",
    "    repo_id=\"mech-interp-uam/llama3.2-1b-sae\",\n",
    "    filename = \"sae.pth\",\n",
    ")\n",
    "print(f\"Checkpoint cached at -> {ckpt_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = (\n",
    "    load_dataset(\n",
    "        \"mech-interp-uam/llama-mlp8-outputs\",\n",
    "        split=\"train\",\n",
    "        # Para el desarrollo, podemos usar streaming=True o False depende de lo\n",
    "        # que sea más cómodo, para los entrenamientos grandes, probablemente sea\n",
    "        # conveniente # streaming=False\n",
    "        # streaming=True,\n",
    "    )\n",
    "    .with_format(\"numpy\")\n",
    "    .batch(MINI_BATCH_SIZE)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = torch.load(ckpt_path, map_location=\"cpu\")\n",
    "\n",
    "# if next(iter(state_dict)).startswith(\"module.\"):\n",
    "#     state_dict = {k.replace(\"module.\", \"\", 1): v\n",
    "#                   for k, v in state_dict.items()}\n",
    "\n",
    "# missing, unexpected = sae.load_state_dict(state_dict, strict=False)\n",
    "# if missing or unexpected:\n",
    "#     print(\"Key mismatch:\", missing, unexpected)\n",
    "\n",
    "#sae.eval() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lógica de encoder directamente aquí, esto debería de hacerse con código en\n",
    "# el módulo de sae, algo así: encoder = Encoder.from_sae_state_dict(state_dict)\n",
    "\n",
    "# En el state dict, debería de haber un (k,v) que nos indique si usó pre_encoder\n",
    "# bias, o fucionarla directamente antes de subirlo a hf\n",
    "\n",
    "\n",
    "W = state_dict['enc.weight']\n",
    "# Si se usó pre_encoder bias, entonces\n",
    "# W(x - b_d) + b_e  = Wx + (b_e - Wb_d)\n",
    "b = state_dict['enc.bias'] - W@state_dict['dec.bias']\n",
    "threshold = state_dict['log_threshold'].exp()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3abd1c3c1d346e7a2fb18f868cbc3d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['activations', 'norm'],\n",
       "    num_rows: 10\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute expected norm\n",
    "norm = (\n",
    "    ds\n",
    "    .shuffle()\n",
    "    .take(10)\n",
    "    .map(lambda row : {\"norm\" : np.linalg.norm(row['activations'], axis=1)})\n",
    "    )\n",
    "list(f for f in ds[\"norm\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "212.4140625\n"
     ]
    }
   ],
   "source": [
    "# Get a placeholder for some sort of identifier that links output vectors of\n",
    "# the mlp with their corresponding token/place in the text used to generate\n",
    "# them\n",
    "\n",
    "from jaxtyping import Array, Float, Int\n",
    "from typing import Iterable\n",
    "\n",
    "rows: Iterable[tuple[Int[Array, \"b\"], Float[Array, \"b b_model\"]]] = enumerate(row['activations'] for row in ds.take(8))\n",
    "\n",
    "l0_moving_average = None\n",
    "for i, row in rows:\n",
    "    # Here we are scaling so that E[||x||] = 1, but this hardcoded value is ugly\n",
    "    # Furtheremore, this 3.4 value is not uptodate, probably training sufered\n",
    "    # because of this oof\n",
    "    x = torch.from_numpy(row)\n",
    "    x /= 3.4\n",
    "    # print(x[0].norm())\n",
    "    # print(x.norm(dim=1).mean())\n",
    "    # print(x.norm(dim=1).size())\n",
    "    x = x@W.T + b\n",
    "    s = Step.apply(x, threshold)\n",
    "    current_l0 = s.mean(0).sum()\n",
    "    if i == 0:\n",
    "        l0_moving_average = current_l0\n",
    "    else:\n",
    "        l0_moving_average = i/(i+1) * l0_moving_average + 1/(i+1) * current_l0\n",
    "\n",
    "    x = s*x\n",
    "print(l0_moving_average.item())\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 843., 1334., 1449.,  ..., 1597., 1748., 2206.])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.sum(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mech_interp.sae import Sae\n",
    "sae = Sae(\n",
    "    d_in = 2048, \n",
    "    d_sae=2048*8, \n",
    "    use_pre_enc_bias=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recon shape: torch.Size([])\n",
      "Latent L0  : 8294.0\n"
     ]
    }
   ],
   "source": [
    "in_dim = sae.enc.in_features\n",
    "dummy = torch.randn(1, in_dim, device = next(sae.parameters()).device)\n",
    "with torch.no_grad():\n",
    "    recon_dict= sae(dummy)\n",
    "print(\"Recon shape:\", recon_dict[\"reconstruction\"].shape)\n",
    "print(\"Latent L0  :\", recon_dict[\"l0\"].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    out = sae(dummy)\n",
    "print(type(out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cargar datos con id's\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "\n",
    "API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "client = OpenAI(api_key=API_KEY) if API_KEY else None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\"\"\n",
    "def simulate_activating_examples_gpt(api_key, n=10) -> list[tuple[int, list[str]]]:\n",
    "   \n",
    "   # Simula ejemplos de activación máxima en frases relacionadas con comunicación.\n",
    "    #Regresa el mismo formato que fetch_max_activating_examples().\n",
    "    \n",
    "    from openai import OpenAI\n",
    "    client = OpenAI(api_key=api_key)\n",
    "\n",
    "    system_prompt = (\n",
    "    \"You are a fluent, context-aware asssistan that wirtes natural, varied, and meaningful sentences\"\n",
    "    \"related to the concept of commnication. Yout output will be used  to study neuron activations in a language model\"\n",
    "    \"Each sentence must contain exactly one significant token related to communication, and that token must be wrapped with double angle brackets like this: <<talk>>\"\n",
    "    \"Avoid generic language or filler. Ensure the token is conceptually central to the sentence's meaning\"\n",
    "    )\n",
    "    user_prompt = (\n",
    "        f\"Generate {n} short sentences (10 to 15 words each) about communication. \"\n",
    "        \"In each sentence, mark exactly one key word related to communication ussing double angle like this: <<talk>>\" \n",
    "        \"Return only the sentences, one per line, with no bullet poinsts, numbering, or explanations\"\n",
    "    )\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt},\n",
    "        ],\n",
    "        max_tokens=300,\n",
    "        temperature=0.7\n",
    "    )\n",
    "\n",
    "    raw_sentences = response.choices[0].message.content.strip().split(\"\\n\")\n",
    "    raw_sentences = [s.strip().lstrip(\"0123456789. \") for s in raw_sentences if s.strip()]\n",
    "\n",
    "    parsed = []\n",
    "    for i, sentence in enumerate(raw_sentences):\n",
    "        tokens = sentence.split()\n",
    "        for j, tok in enumerate(tokens):\n",
    "            if tok.startswith(\"<<\") and tok.endswith(\">>\"):\n",
    "                tokens[j] = f\"<<{tok.strip('<>')}>>\"\n",
    "                break\n",
    "        parsed.append((i, tokens))\n",
    "\n",
    "    return parsed\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "def create_prompt_gpt4o_from_simulated(examples: list[tuple[int, list[str]]], use_chain_of_thought=True) -> dict[str, str]:\n",
    "    \n",
    "    #Construye un prompt de GPT-4o a partir de ejemplos simulados con tokens activadores marcados.\n",
    "    \n",
    "    formatted = \"\\n\".join(f\"{i+1}. {' '.join(tokens)}\" for i, tokens in examples)\n",
    "\n",
    "    system_prompt = (\n",
    "        \"You are analyzing a latent neuron in a transformer-based language model. \"\n",
    "        \"Each sentence below contains one token that strongly activates this latent neuron, highlighted using << >>. \"\n",
    "        \"Your task is to interpret what concept, theme, or category this neuron responds to. \"\n",
    "        \"Be precise but not overly narrow. Use fewer than 20 words. \"\n",
    "        \"Avoid punctuation, lists, formatting, or generic labels like 'words' or 'nouns'. \"\n",
    "        \"Focus on shared semantic meaning across the highlighted tokens.\"\n",
    "    )\n",
    "\n",
    "    assistant_prompt = (\n",
    "        \"Start by mentally grouping the highlighted tokens into a conceptual category. \"\n",
    "        \"Then, write a final interpretation in fewer than 20 words. \"\n",
    "        \"This neuron activates on\"\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"system\": system_prompt,\n",
    "        \"user\": f\"The activating examples are:\\n\\n{formatted}\",\n",
    "        \"assistant\": assistant_prompt,\n",
    "    }\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gpt4o_explanation_from_prompt(prompts: dict, n_completions=3, max_tokens=100) -> list[str]:\n",
    "    \"\"\"\n",
    "    Llama a la API de GPT-4o con un prompt ya generado y devuelve las interpretaciones.\n",
    "    \"\"\"\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": prompts[\"system\"]},\n",
    "            {\"role\": \"user\", \"content\": prompts[\"user\"]},\n",
    "            {\"role\": \"assistant\", \"content\": prompts[\"assistant\"]},\n",
    "        ],\n",
    "        n=n_completions,\n",
    "        max_tokens=max_tokens,\n",
    "        temperature=0.7,\n",
    "    )\n",
    "    return [choice.message.content.strip() for choice in response.choices]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OPENAI_API_KEY no está configurada.\n"
     ]
    }
   ],
   "source": [
    "if API_KEY:\n",
    "    #simulated_examples = simulate_activating_examples_gpt(api_key=API_KEY, n=10)\n",
    "    #prompt = create_prompt_gpt4o_from_simulated(simulated_examples, use_chain_of_thought=True)\n",
    "    explanations = get_gpt4o_explanation_from_prompt(prompt, n_completions=3)\n",
    "\n",
    "    for i, exp in enumerate(explanations):\n",
    "        print(f\"[Explicación {i+1}]: {exp}\")\n",
    "else:\n",
    "    print(\"OPENAI_API_KEY no está configurada.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
