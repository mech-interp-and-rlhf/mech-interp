{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrenamiento de un SAE sobre las salidad y activaciones de la MLP intermedia de llama3.2 1B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Costo de entrenamiento\n",
    "\n",
    "Generalmente, el costo computacional de un LLM está dominado por la evaluación\n",
    "de sus MLPs (referencia a el seminario en una universidad de un ex empleado\n",
    "de anthropic)\n",
    "\n",
    "Una aprocimación simple del costo de entrenamiento de un modelo es\n",
    "\n",
    "$$\n",
    "  6ND \n",
    "$$\n",
    "esto en términos de FLOPs (operaciones de punto flotante).\n",
    "(citar a chinchilla scaling laws)\n",
    "\n",
    "donde $N$ es el número de parámetros y $D$ la cantidad de muestras en el\n",
    "conjunto de entrenamiento.\n",
    "\n",
    "Esto se debe a que, generalmente, cada parámetro actua en una multiplicaciónl\n",
    "y en una suma de punto flotante, dandonos un costo de $2ND$ tan solo en el\n",
    "forward pass. Tipicamente, el costo de el backward pass es el doble del forward\n",
    "pass, haciendo que su costo sea $4ND$. Sumando tenemos el resultado previamente\n",
    "mencionado.\n",
    "\n",
    "Sea $N_l$ el número de parámetros de llama.\n",
    "Como nosotros vamos a entrenar un autoencoder sobre un MLP a la mitad de llama,\n",
    "solo necesitamos evaluar esa primera mitad. Además, no correremos el backward\n",
    "pass sobre los parámetros de llama, pues no buscamos modificarlos, es decir, los\n",
    "mantendrémos fijos. Por esto, tenemos que el FLOPs realizados por tal mitad del\n",
    "modelo llama es\n",
    "\n",
    "$$\n",
    "  N_l D\n",
    "$$\n",
    "\n",
    "En cuanto al SAE, solo considerando el costo de aplicar sus matrices, tenemos\n",
    "\n",
    "$$\n",
    "  6 (2d_\\text{in}d_\\text{sae})D\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el caso de gemmascope, entre todos los SAEs que entrenaron, los más pequeños\n",
    "entrenados en las salidas de las MLPs, se entrenaron en 4 mil millones de\n",
    "vectores de activaciones, con la dimencion de los vectores en el stream\n",
    "recidual (y por lo tanto, de la salida de las capas MLP), es 2048, con\n",
    "$d_\\text{sae} = 2028 * 8$.\n",
    "\n",
    "Deseamos encontrar hiperparámetros para entrenar SAEs, para esto:\n",
    "- Usamos una primera aproximación razonable modificando los hiperparámetros para\n",
    "  los autoencoders más pequeños entrenados en salidas de las MLPs de gemma 2\n",
    "  2 B\n",
    "- Ajustamos una power law en base a 2 entrenamientos de SAEs más pequeñas,\n",
    "  usando el mismo learning rate.\n",
    "- Ajustamos una power law para el learning rate con los hiperparámetos optimos\n",
    "  que estimó el paso anterior.\n",
    "\n",
    "Si ignoramos la relación posicional de los tokens y asumimos una distribución\n",
    "uniforme, tenemos que la entropía por token es\n",
    "\n",
    "$$\n",
    "  \\log_2 (\\text{tamaño del vocabulario})\n",
    "$$\n",
    "ya que el vocabulario de gemma2 2B es 256000 y el de llama es 128000, obtenemos\n",
    "que el número de tokens equivalente sería 4.2 mil millones. Ya que nuestro\n",
    "modelo es la mitad de tamaño de gemma2 2B, una primera cantidad de datos\n",
    "razonable para entrenar nuesto sae más grande es $2.1B$\n",
    "\n",
    "En el caso de llama3.2 1B, eso resultaría en\n",
    "\n",
    "$$\n",
    "  N_l D = (2.1 \\times 10^9)(1.2 \\times 10^9) \\approx 2.5 \\times 10^{18}\n",
    "$$\n",
    "Una RTX 4090 puede realisar cada segundo un máximo de $165 \\times 10^{12}$\n",
    "operaciones con tensores de 16 bits y acumulador de 32 bits (referencia\n",
    "al reporte técnico v1.0.1), luego, estimamos 4.2 horas de entrenamiento\n",
    "tan solo considerando la computación en el modelo llama.\n",
    "\n",
    "Ahora, para estimar las horas-RTX4090 para el autoencoder, en el caso de\n",
    "entrenarlo en la salida de la MLP intermedia, tendríamos\n",
    "\n",
    "$$\n",
    "    6(2.1 \\times 10^9)(2048^2)(8)(2) = 8.5 \\times 10^{17}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# El modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import jaxtyping\n",
    "import dataclasses\n",
    "from tqdm import tqdm\n",
    "from torch.nn import functional as F\n",
    "from torch.linalg import vector_norm\n",
    "import math\n",
    "import datasets\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype = torch.float32\n",
    "device = \"cuda\" if torch.cuda.is_available else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Step(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, threshold):\n",
    "        ctx.save_for_backward(x, threshold)\n",
    "        return (x > threshold).to(x.dtype)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        bandwidth = 0.001\n",
    "        x, threshold = ctx.saved_tensors\n",
    "\n",
    "        grad_threshold = torch.where(\n",
    "            abs(F.relu(x) - threshold) < bandwidth/2,\n",
    "            -1.0/bandwidth, 0)\n",
    "        \n",
    "        return torch.zeros_like(x), grad_threshold * grad_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sae(nn.Module):\n",
    "    def __init__(self, d_in, d_sae, use_pre_enc_bias=True, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.enc = nn.Linear(d_in, d_sae, dtype=dtype)\n",
    "        self.dec = nn.Linear(d_sae, d_in, dtype=dtype)\n",
    "        with torch.no_grad():\n",
    "            # normalize each of the d_sae dictonary vectors\n",
    "            self.dec.weight /= vector_norm(self.dec.weight, dim=1, keepdim=True)\n",
    "            self.enc.weight.copy_(self.dec.weight.clone().t())\n",
    "            self.enc.bias.copy_(torch.zeros_like(self.enc.bias))\n",
    "            self.dec.bias.copy_(torch.zeros_like(self.dec.bias))\n",
    "        self.log_threshold = nn.Parameter(\n",
    "            torch.log(torch.full((d_sae,), 0.001, dtype=dtype)))\n",
    "        self.use_pre_enc_bias = use_pre_enc_bias\n",
    "        def project_out_parallel_grad(dim, tensor):\n",
    "            @torch.no_grad\n",
    "            def hook(grad_in):\n",
    "                # norm along dim=dim of the tensor is assumed to be 1 as we\n",
    "                # are going to normalize it after every grad update\n",
    "                dot = (tensor * grad_in).sum(dim=dim, keepdim=True)\n",
    "                return grad_in - dot * tensor\n",
    "            return hook\n",
    "\n",
    "        self.dec.weight.register_hook(\n",
    "            project_out_parallel_grad(1, self.dec.weight))\n",
    "                \n",
    "\n",
    "    def forward(self,\n",
    "        x,\n",
    "        return_mask=False,\n",
    "        return_l0=True,\n",
    "        return_reconstruction_loss=True,\n",
    "    ):\n",
    "        \"We compute this much here so that compile() can do its magic\"\n",
    "        # as per train_gpt2.py on karpathy's llm.c repo, there are performance\n",
    "        # reasons not to return stuff\n",
    "        d = {}\n",
    "        original_input = x\n",
    "        if self.use_pre_enc_bias:\n",
    "            x = x - self.dec.bias\n",
    "        \n",
    "        x = self.enc(x)\n",
    "        threshold = torch.exp(self.log_threshold)\n",
    "        s = Step.apply(x, threshold)\n",
    "        if return_mask:\n",
    "            d['mask'] = s\n",
    "        if return_l0:\n",
    "            d['l0'] = s.float().mean(0).sum(-1)\n",
    "        if not return_reconstruction_loss:\n",
    "            return d\n",
    "        x = x*s\n",
    "        x = self.dec(x)\n",
    "\n",
    "        d['reconstruction'] = ((x - original_input).pow(2)).mean(0).sum()\n",
    "\n",
    "        return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_schedule_with_warmup(\n",
    "    current_step: int,\n",
    "    warmup_steps: int,\n",
    "    total_steps: int\n",
    "    ):\n",
    "    if current_step < warmup_steps:\n",
    "        lr =  (1 + current_step) / warmup_steps\n",
    "        return lr\n",
    "    progress = (current_step - warmup_steps) / (total_steps - warmup_steps)\n",
    "    lr =  0.5 * (1 + math.cos(math.pi * progress))\n",
    "    return lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparsity_schedule(step, warmup_steps, max_sparsity_coeff):\n",
    "    if step >= warmup_steps:\n",
    "        return max_sparsity_coeff\n",
    "    return max_sparsity_coeff*((step+1) / warmup_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded successfully: 668501 examples\n",
      "Features: {'activations': Sequence(feature=Value(dtype='float16', id=None), length=2048, id=None)}\n",
      "First example shape: (2048,)\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the dataset\n",
    "ds = load_dataset('mech-interp-uam/llama-mlp8-outputs')\n",
    "ds.set_format('numpy')\n",
    "\n",
    "# Check the dataset structure\n",
    "print(f\"Dataset loaded successfully: {len(ds['train'])} examples\")\n",
    "print(f\"Features: {ds['train'].features}\")\n",
    "print(f\"First example shape: {ds['train'][0]['activations'].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9ebaa1ae5524f358c7bd4169edadb59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.4179318\n"
     ]
    }
   ],
   "source": [
    "B = 1024\n",
    "n = 100\n",
    "sample_norms = (\n",
    "    ds['train']\n",
    "    .batch(B)\n",
    "    .shuffle()\n",
    "    .take(n)\n",
    "    .map(lambda row: {\"norm\": np.linalg.norm(row[\"activations\"], axis=1).mean()},\n",
    "         remove_columns=['activations'])\n",
    ")\n",
    "norm = None\n",
    "for i, sample_norm in enumerate(sample_norms):\n",
    "    current_norm = sample_norm['norm']\n",
    "    if i == 0:\n",
    "        norm = current_norm\n",
    "        continue\n",
    "    norm = i/(i+1) * norm + 1/(i+1) * current_norm\n",
    "\n",
    "print(norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class ActivationsDataset(Dataset):\n",
    "    def __init__(self, hf_dataset):\n",
    "        self.data = hf_dataset\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Return as float16, on modern GPUs conversion from float 16 to 32 is\n",
    "        # free compared to matmults or so I was told\n",
    "        return torch.tensor(self.data[idx]['activations'])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = 1024\n",
    "dataset = ActivationsDataset(ds['train'])\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=batch,\n",
    "    shuffle=True,\n",
    "    num_workers=12,\n",
    "    pin_memory=True,\n",
    "    prefetch_factor=32,\n",
    "    persistent_workers=True,\n",
    "    drop_last=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "652"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/652 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reconstruction=0.31900954246520996\n",
      "l0=7344.169921875\n",
      "sparsity_coefficient=3.1250000000000003e-09\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 652/652 [00:18<00:00, 35.56it/s]\n",
      "100%|██████████| 652/652 [00:15<00:00, 41.91it/s]\n",
      "100%|██████████| 652/652 [00:15<00:00, 42.90it/s]\n",
      "100%|██████████| 652/652 [00:15<00:00, 42.56it/s]\n",
      "100%|██████████| 652/652 [00:15<00:00, 41.93it/s]\n",
      "100%|██████████| 652/652 [00:15<00:00, 40.76it/s]\n",
      "100%|██████████| 652/652 [00:15<00:00, 41.09it/s]\n",
      " 68%|██████▊   | 446/652 [00:10<00:04, 44.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reconstruction=0.042265087366104126\n",
      "l0=9552.5205078125\n",
      "sparsity_coefficient=1.5628125000000002e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 652/652 [00:15<00:00, 43.01it/s]\n",
      "100%|██████████| 652/652 [00:14<00:00, 43.51it/s]\n",
      "100%|██████████| 652/652 [00:15<00:00, 42.13it/s]\n",
      "100%|██████████| 652/652 [00:15<00:00, 41.18it/s]\n",
      "100%|██████████| 652/652 [00:16<00:00, 40.18it/s]\n",
      "100%|██████████| 652/652 [00:15<00:00, 42.12it/s]\n",
      "100%|██████████| 652/652 [00:15<00:00, 42.23it/s]\n",
      "100%|██████████| 652/652 [00:17<00:00, 38.21it/s]\n",
      " 33%|███▎      | 217/652 [00:05<00:09, 44.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reconstruction=0.039009954780340195\n",
      "l0=7292.310546875\n",
      "sparsity_coefficient=3.1253125e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 652/652 [00:15<00:00, 43.01it/s]\n",
      "100%|██████████| 652/652 [00:15<00:00, 42.83it/s]\n",
      "100%|██████████| 652/652 [00:15<00:00, 42.27it/s]\n",
      "100%|██████████| 652/652 [00:15<00:00, 43.33it/s]\n",
      "100%|██████████| 652/652 [00:15<00:00, 41.72it/s]\n",
      "100%|██████████| 652/652 [00:15<00:00, 41.72it/s]\n",
      "100%|██████████| 652/652 [00:16<00:00, 40.35it/s]\n",
      "100%|██████████| 652/652 [00:15<00:00, 41.21it/s]\n",
      "  0%|          | 1/652 [00:00<04:02,  2.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reconstruction=0.042998429387807846\n",
      "l0=4478.564453125\n",
      "sparsity_coefficient=4.6878125e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 652/652 [00:15<00:00, 42.87it/s]\n",
      "100%|██████████| 652/652 [00:17<00:00, 37.81it/s]\n",
      "100%|██████████| 652/652 [00:16<00:00, 40.50it/s]\n",
      "100%|██████████| 652/652 [00:15<00:00, 41.91it/s]\n",
      "100%|██████████| 652/652 [00:15<00:00, 41.87it/s]\n",
      "100%|██████████| 652/652 [00:15<00:00, 42.95it/s]\n",
      "100%|██████████| 652/652 [00:14<00:00, 43.98it/s]\n",
      " 68%|██████▊   | 446/652 [00:10<00:04, 42.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reconstruction=0.06718900799751282\n",
      "l0=2394.63671875\n",
      "sparsity_coefficient=6.250312500000001e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 652/652 [00:14<00:00, 44.05it/s]\n",
      "100%|██████████| 652/652 [00:16<00:00, 40.31it/s]\n",
      "100%|██████████| 652/652 [00:15<00:00, 41.24it/s]\n",
      "100%|██████████| 652/652 [00:15<00:00, 42.58it/s]\n",
      "100%|██████████| 652/652 [00:15<00:00, 41.32it/s]\n",
      "100%|██████████| 652/652 [00:15<00:00, 42.90it/s]\n",
      "100%|██████████| 652/652 [00:14<00:00, 43.77it/s]\n",
      "100%|██████████| 652/652 [00:15<00:00, 43.44it/s]\n",
      " 35%|███▌      | 230/652 [00:05<00:10, 41.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reconstruction=0.08861327171325684\n",
      "l0=1413.4609375\n",
      "sparsity_coefficient=7.812812500000001e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 652/652 [00:15<00:00, 41.48it/s]\n",
      "100%|██████████| 652/652 [00:15<00:00, 41.87it/s]\n",
      "100%|██████████| 652/652 [00:15<00:00, 43.08it/s]\n",
      "100%|██████████| 652/652 [00:15<00:00, 41.33it/s]\n",
      "100%|██████████| 652/652 [00:15<00:00, 42.72it/s]\n",
      "100%|██████████| 652/652 [00:15<00:00, 41.82it/s]\n",
      "100%|██████████| 652/652 [00:15<00:00, 43.17it/s]\n",
      "100%|██████████| 652/652 [00:16<00:00, 38.96it/s]\n",
      "  2%|▏         | 13/652 [00:00<00:26, 24.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reconstruction=0.09703446924686432\n",
      "l0=1044.912109375\n",
      "sparsity_coefficient=9.375312500000001e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 652/652 [00:15<00:00, 41.06it/s]\n",
      "100%|██████████| 652/652 [00:15<00:00, 42.41it/s]\n",
      "100%|██████████| 652/652 [00:16<00:00, 40.27it/s]\n",
      "100%|██████████| 652/652 [00:16<00:00, 40.56it/s]\n",
      "100%|██████████| 652/652 [00:15<00:00, 41.12it/s]\n",
      "100%|██████████| 652/652 [00:16<00:00, 40.22it/s]\n",
      "100%|██████████| 652/652 [00:15<00:00, 42.30it/s]\n",
      " 69%|██████▉   | 451/652 [00:10<00:04, 46.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reconstruction=0.10535077750682831\n",
      "l0=893.7041015625\n",
      "sparsity_coefficient=0.000109378125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 652/652 [00:15<00:00, 41.90it/s]\n",
      "100%|██████████| 652/652 [00:15<00:00, 43.22it/s]\n",
      "100%|██████████| 652/652 [00:15<00:00, 41.87it/s]\n",
      "100%|██████████| 652/652 [00:15<00:00, 43.23it/s]\n",
      "100%|██████████| 652/652 [00:15<00:00, 41.26it/s]\n",
      "100%|██████████| 652/652 [00:16<00:00, 39.94it/s]\n",
      "100%|██████████| 652/652 [00:15<00:00, 42.38it/s]\n",
      "100%|██████████| 652/652 [00:15<00:00, 41.55it/s]\n",
      " 35%|███▌      | 229/652 [00:05<00:10, 40.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reconstruction=0.1057296097278595\n",
      "l0=784.95703125\n",
      "sparsity_coefficient=0.000125003125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 652/652 [00:15<00:00, 42.76it/s]\n",
      "100%|██████████| 652/652 [00:15<00:00, 42.99it/s]\n",
      "100%|██████████| 652/652 [00:15<00:00, 41.62it/s]\n",
      "100%|██████████| 652/652 [00:15<00:00, 42.19it/s]\n",
      "100%|██████████| 652/652 [00:15<00:00, 42.16it/s]\n",
      "100%|██████████| 652/652 [00:14<00:00, 43.85it/s]\n",
      "100%|██████████| 652/652 [00:15<00:00, 42.40it/s]\n",
      "100%|██████████| 652/652 [00:15<00:00, 42.54it/s]\n",
      "  2%|▏         | 13/652 [00:00<00:26, 24.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reconstruction=0.10894829034805298\n",
      "l0=717.8662109375\n",
      "sparsity_coefficient=0.000140628125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 652/652 [00:16<00:00, 39.11it/s]\n",
      "100%|██████████| 652/652 [00:15<00:00, 41.33it/s]\n",
      "100%|██████████| 652/652 [00:17<00:00, 36.68it/s]\n",
      "100%|██████████| 652/652 [00:15<00:00, 42.32it/s]\n",
      "100%|██████████| 652/652 [00:14<00:00, 43.68it/s]\n",
      "100%|██████████| 652/652 [00:15<00:00, 42.40it/s]\n",
      "100%|██████████| 652/652 [00:15<00:00, 40.95it/s]\n",
      " 68%|██████▊   | 445/652 [00:11<00:06, 32.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reconstruction=0.11511114239692688\n",
      "l0=664.91015625\n",
      "sparsity_coefficient=0.000156253125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 652/652 [00:16<00:00, 40.35it/s]\n",
      "100%|██████████| 652/652 [00:16<00:00, 40.05it/s]\n",
      "100%|██████████| 652/652 [00:15<00:00, 43.34it/s]\n",
      "100%|██████████| 652/652 [00:15<00:00, 43.07it/s]\n",
      "100%|██████████| 652/652 [00:15<00:00, 42.30it/s]\n",
      "100%|██████████| 652/652 [00:15<00:00, 41.77it/s]\n",
      "100%|██████████| 652/652 [00:16<00:00, 39.81it/s]\n",
      "100%|██████████| 652/652 [00:15<00:00, 42.89it/s]\n",
      " 37%|███▋      | 241/652 [00:06<00:09, 43.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reconstruction=0.11739729344844818\n",
      "l0=618.78125\n",
      "sparsity_coefficient=0.000171878125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 652/652 [00:15<00:00, 41.06it/s]\n",
      "100%|██████████| 652/652 [00:15<00:00, 42.97it/s]\n",
      "100%|██████████| 652/652 [00:15<00:00, 42.32it/s]\n",
      "100%|██████████| 652/652 [00:15<00:00, 41.74it/s]\n",
      "100%|██████████| 652/652 [00:15<00:00, 41.19it/s]\n",
      "100%|██████████| 652/652 [00:16<00:00, 38.38it/s]\n",
      "100%|██████████| 652/652 [00:15<00:00, 42.82it/s]\n",
      "100%|██████████| 652/652 [00:15<00:00, 42.30it/s]\n",
      "  4%|▍         | 25/652 [00:00<00:18, 33.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reconstruction=0.11883846670389175\n",
      "l0=576.2802734375\n",
      "sparsity_coefficient=0.000187503125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 652/652 [00:15<00:00, 40.98it/s]\n",
      "100%|██████████| 652/652 [00:16<00:00, 40.33it/s]\n",
      "100%|██████████| 652/652 [00:15<00:00, 41.97it/s]\n",
      "100%|██████████| 652/652 [00:15<00:00, 42.94it/s]\n",
      "100%|██████████| 652/652 [00:17<00:00, 38.04it/s]\n",
      "  9%|▉         | 61/652 [00:01<00:14, 39.87it/s]Exception ignored in: Exception ignored in: Exception ignored in: <function tqdm.__del__ at 0x7f3a2f164b80><function tqdm.__del__ at 0x7f3a2f164b80><function tqdm.__del__ at 0x7f3a2f164b80>\n",
      "\n",
      "\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/venv/main/lib/python3.12/site-packages/tqdm/std.py\", line 1148, in __del__\n",
      "  File \"/venv/main/lib/python3.12/site-packages/tqdm/std.py\", line 1148, in __del__\n",
      "  File \"/venv/main/lib/python3.12/site-packages/tqdm/std.py\", line 1148, in __del__\n",
      "            self.close()self.close()self.close()\n",
      "\n",
      "\n",
      "  File \"/venv/main/lib/python3.12/site-packages/tqdm/notebook.py\", line 273, in close\n",
      "  File \"/venv/main/lib/python3.12/site-packages/tqdm/notebook.py\", line 273, in close\n",
      "  File \"/venv/main/lib/python3.12/site-packages/tqdm/notebook.py\", line 273, in close\n",
      "            if self.disable:if self.disable:if self.disable:\n",
      "\n",
      "\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\n",
      "^AttributeError\n",
      "AttributeError: AttributeError: 'tqdm' object has no attribute 'disable': 'tqdm' object has no attribute 'disable'\n",
      "'tqdm' object has no attribute 'disable'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#torch.set_float32_matmul_precision('high')\n",
    "steps = 2**16\n",
    "max_lr = 7e-5\n",
    "d_in = 2048\n",
    "d_sae = 2048*8\n",
    "model = Sae(d_in, d_sae)\n",
    "model.to('cuda')\n",
    "model.compile()\n",
    "warmup_steps=1000\n",
    "sparcity_warmup_steps=128000\n",
    "total_steps=128000 #for now\n",
    "optimizer = torch.optim.Adam([\n",
    "    {\"params\": model.enc.parameters(), \"lr\":   max_lr, \"betas\":(0.0, 0.999)},\n",
    "    {\"params\": model.dec.parameters(), \"lr\":   max_lr, \"betas\":(0.0, 0.999)},\n",
    "    {\"params\": model.log_threshold,    \"lr\": 2*max_lr, \"betas\":(0.99,0.999)},\n",
    "])\n",
    "max_sparsity_coeff = 0.0004\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(\n",
    "    optimizer,\n",
    "    lr_lambda=lambda step: cosine_schedule_with_warmup(step, warmup_steps, total_steps)\n",
    ")\n",
    "# train_ds = fake_train_loader(batch, d_in, total_steps)\n",
    "total_step = 0\n",
    "should_break = False\n",
    "for epoch in range(1000):\n",
    "    for step, x in enumerate(tqdm(dataloader)):\n",
    "        x = x.to(\"cuda\", non_blocking=True).to(dtype)\n",
    "        x /= 3.4 # this is supposed to be the expected norm\n",
    "        optimizer.zero_grad()\n",
    "        d = model(x)\n",
    "        reconstruction_loss, l0 = d['reconstruction'], d['l0']\n",
    "        sparsity_coefficient = sparsity_schedule(total_step, sparcity_warmup_steps, max_sparsity_coeff)\n",
    "        loss = reconstruction_loss + sparsity_coefficient * l0\n",
    "        # log losses, compute stats, etc\n",
    "        grad = loss.backward()\n",
    "        # norm = torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        # metrics\n",
    "        if (total_step % 5000) == 0:\n",
    "            with torch.no_grad():\n",
    "                # print metrics\n",
    "                print(f\"reconstruction={reconstruction_loss.item()}\")\n",
    "                print(f\"l0={l0.item()}\")\n",
    "                # print(f\"norm={norm.item()}\")\n",
    "                print(f\"{sparsity_coefficient=}\")\n",
    "        optimizer.step()\n",
    "        # TODO: sparsity_coefficient scheduler\n",
    "        # print(scheduler.get_lr())\n",
    "        scheduler.step()\n",
    "\n",
    "        # normalize\n",
    "        with torch.no_grad():\n",
    "            wdecnorm = vector_norm(model.dec.weight, dim=1, keepdim=True)\n",
    "            model.dec.weight /= wdecnorm\n",
    "    # print(f\"epoch loss: {loss.detach().item()}\")\n",
    "        total_step +=1\n",
    "        if total_step > total_steps:\n",
    "            should_break = True\n",
    "            break\n",
    "    if should_break:\n",
    "        break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"/workspace/llama3.2-1b-sae/sae.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sae2 = Sae(d_in, d_sae)\n",
    "sae2.load_state_dict(model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sa"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
