{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrenamiento de un SAE sobre las salidad y activaciones de la MLP intermedia de llama3.2 1B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Costo de entrenamiento\n",
    "\n",
    "Generalmente, el costo computacional de un LLM está dominado por la evaluación\n",
    "de sus MLPs (referencia a el seminario en una universidad de un ex empleado\n",
    "de anthropic)\n",
    "\n",
    "Una aprocimación simple del costo de entrenamiento de un modelo es\n",
    "\n",
    "$$\n",
    "  6ND \n",
    "$$\n",
    "esto en términos de FLOPs (operaciones de punto flotante).\n",
    "(citar a chinchilla scaling laws)\n",
    "\n",
    "donde $N$ es el número de parámetros y $D$ la cantidad de muestras en el\n",
    "conjunto de entrenamiento.\n",
    "\n",
    "Esto se debe a que, generalmente, cada parámetro actua en una multiplicaciónl\n",
    "y en una suma de punto flotante, dandonos un costo de $2ND$ tan solo en el\n",
    "forward pass. Tipicamente, el costo de el backward pass es el doble del forward\n",
    "pass, haciendo que su costo sea $4ND$. Sumando tenemos el resultado previamente\n",
    "mencionado.\n",
    "\n",
    "Sea $N_l$ el número de parámetros de llama.\n",
    "Como nosotros vamos a entrenar un autoencoder sobre un MLP a la mitad de llama,\n",
    "solo necesitamos evaluar esa primera mitad. Además, no correremos el backward\n",
    "pass sobre los parámetros de llama, pues no buscamos modificarlos, es decir, los\n",
    "mantendrémos fijos. Por esto, tenemos que el FLOPs realizados por tal mitad del\n",
    "modelo llama es\n",
    "\n",
    "$$\n",
    "  N_l D\n",
    "$$\n",
    "\n",
    "En cuanto al SAE, solo considerando el costo de aplicar sus matrices, tenemos\n",
    "\n",
    "$$\n",
    "  6 (2d_\\text{in}d_\\text{sae})D\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el caso de gemmascope, entre todos los SAEs que entrenaron, los más pequeños\n",
    "entrenados en las salidas de las MLPs, se entrenaron en 4 mil millones de\n",
    "vectores de activaciones, con la dimencion de los vectores en el stream\n",
    "recidual (y por lo tanto, de la salida de las capas MLP), es 2048, con\n",
    "$d_\\text{sae} = 2028 * 8$.\n",
    "\n",
    "Deseamos encontrar hiperparámetros para entrenar SAEs, para esto:\n",
    "- Usamos una primera aproximación razonable modificando los hiperparámetros para\n",
    "  los autoencoders más pequeños entrenados en salidas de las MLPs de gemma 2\n",
    "  2 B\n",
    "- Ajustamos una power law en base a 2 entrenamientos de SAEs más pequeñas,\n",
    "  usando el mismo learning rate.\n",
    "- Ajustamos una power law para el learning rate con los hiperparámetos optimos\n",
    "  que estimó el paso anterior.\n",
    "\n",
    "Si ignoramos la relación posicional de los tokens y asumimos una distribución\n",
    "uniforme, tenemos que la entropía por token es\n",
    "\n",
    "$$\n",
    "  \\log_2 (\\text{tamaño del vocabulario})\n",
    "$$\n",
    "ya que el vocabulario de gemma2 2B es 256000 y el de llama es 128000, obtenemos\n",
    "que el número de tokens equivalente sería 4.2 mil millones. Ya que nuestro\n",
    "modelo es la mitad de tamaño de gemma2 2B, una primera cantidad de datos\n",
    "razonable para entrenar nuesto sae más grande es $2.1B$\n",
    "\n",
    "En el caso de llama3.2 1B, eso resultaría en\n",
    "\n",
    "$$\n",
    "  N_l D = (2.1 \\times 10^9)(1.2 \\times 10^9) \\approx 2.5 \\times 10^{18}\n",
    "$$\n",
    "Una RTX 4090 puede realisar cada segundo un máximo de $165 \\times 10^{12}$\n",
    "operaciones con tensores de 16 bits y acumulador de 32 bits (referencia\n",
    "al reporte técnico v1.0.1), luego, estimamos 4.2 horas de entrenamiento\n",
    "tan solo considerando la computación en el modelo llama.\n",
    "\n",
    "Ahora, para estimar las horas-RTX4090 para el autoencoder, en el caso de\n",
    "entrenarlo en la salida de la MLP intermedia, tendríamos\n",
    "\n",
    "$$\n",
    "    6(2.1 \\times 10^9)(2048^2)(8)(2) = 8.5 \\times 10^{17}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# El modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import jaxtyping\n",
    "import dataclasses\n",
    "from tqdm import tqdm\n",
    "from torch.nn import functional as F\n",
    "from torch.linalg import vector_norm\n",
    "import math\n",
    "import datasets\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Step(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, threshold):\n",
    "        ctx.save_for_backward(x, threshold)\n",
    "        return (x > threshold).to(x.dtype)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        bandwidth = 0.001\n",
    "        x, threshold = ctx.saved_tensors\n",
    "\n",
    "        grad_threshold = torch.where(\n",
    "            abs(F.relu(x) - threshold) < bandwidth/2,\n",
    "            -1.0/bandwidth, 0)\n",
    "        \n",
    "        return torch.zeros_like(x), grad_threshold * grad_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype = torch.float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sae(nn.Module):\n",
    "    def __init__(self, d_in, d_sae, use_pre_enc_bias=True, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.enc = nn.Linear(d_in, d_sae, dtype=dtype)\n",
    "        self.dec = nn.Linear(d_sae, d_in, dtype=dtype)\n",
    "        with torch.no_grad():\n",
    "            # normalize each of the d_sae dictonary vectors\n",
    "            self.dec.weight /= vector_norm(self.dec.weight, dim=0, keepdim=True)\n",
    "            self.enc.weight.copy_(self.dec.weight.clone().t())\n",
    "            self.enc.bias.copy_(torch.zeros_like(self.enc.bias))\n",
    "            self.dec.bias.copy_(torch.zeros_like(self.dec.bias))\n",
    "        self.log_threshold = nn.Parameter(\n",
    "            torch.log(torch.full((d_sae,), 0.001, dtype=dtype)))\n",
    "        self.use_pre_enc_bias = use_pre_enc_bias\n",
    "        def project_out_parallel_grad(dim, tensor):\n",
    "            @torch.no_grad\n",
    "            def hook(grad_in):\n",
    "                # norm along dim=dim of the tensor is assumed to be 1 as we\n",
    "                # are going to normalize it after every grad update\n",
    "                dot = (tensor * grad_in).sum(dim=dim, keepdim=True)\n",
    "                return grad_in - dot * tensor\n",
    "            return hook\n",
    "\n",
    "        self.dec.weight.register_hook(\n",
    "            project_out_parallel_grad(0, self.dec.weight))\n",
    "                \n",
    "\n",
    "    def forward(self,\n",
    "        x,\n",
    "        return_mask=False,\n",
    "        return_l0=True,\n",
    "        return_reconstruction_loss=True,\n",
    "    ):\n",
    "        \"We compute this much here so that compile() can do its magic\"\n",
    "        # as per train_gpt2.py on karpathy's llm.c repo, there are performance\n",
    "        # reasons not to return stuff\n",
    "        d = {}\n",
    "        original_input = x\n",
    "        if self.use_pre_enc_bias:\n",
    "            x = x - self.dec.bias\n",
    "        \n",
    "        x = self.enc(x)\n",
    "        threshold = torch.exp(self.log_threshold)\n",
    "        s = Step.apply(x, threshold)\n",
    "        if return_mask:\n",
    "            d['mask'] = s\n",
    "        if return_l0:\n",
    "            d['l0'] = s.float().mean(0).sum(-1)\n",
    "        if not return_reconstruction_loss:\n",
    "            return d\n",
    "        x = x*s\n",
    "        x = self.dec(x)\n",
    "        # print(((((x - original_input).float())**2) == 0).sum().to(int))\n",
    "\n",
    "        # d['reconstruction'] = ((x - original_input)**2).mean(0, dtype=torch.float32).sum()\n",
    "        d['reconstruction'] = ((x.float() - original_input.float()).pow(2)).mean(0).sum()\n",
    "        # use pow(2)?\n",
    "        # d['reconstruction'] = ((x - original_input)**2).sum(1, dtype=torch.float32).mean()\n",
    "        # d['reconstruction'] = (torch.linalg.vector_norm(x - original_input, dim=1, dtype=torch.float32)**2).mean()\n",
    "\n",
    "        return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# d_in = 1024\n",
    "# d_sae = d_in*8\n",
    "# sae = Sae(d_in, d_sae)\n",
    "# sae.to(\"cuda\")\n",
    "# sae.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# b = 512\n",
    "# x = torch.randn(b, d_in)\n",
    "# x = x.to(torch.float16).to(\"cuda\")\n",
    "# # x /= x.norm(dim=1, keepdim=True).mean(dim=1, keepdim=True)\n",
    "# x /= torch.linalg.vector_norm(x, dim=1, keepdim=True).mean(dim=1, keepdim=True)\n",
    "# x = x.to(torch.float16)\n",
    "# d = sae(x)\n",
    "# d['reconstruction'].detach().cpu().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collection = []\n",
    "# for i in range(20):\n",
    "#     if not (i % 10):\n",
    "#         print(i)\n",
    "#     x = torch.randn(b, d_in)\n",
    "#     # x /= x.norm(dim=1, keepdim=True).mean(dim=1, keepdim=True)\n",
    "#     x /= torch.linalg.vector_norm(x, dim=1, keepdim=True).mean(dim=1, keepdim=True)\n",
    "#     x = x.half()\n",
    "#     sae = Sae(d_in, d_sae)\n",
    "#     if not sae.enc.weight.detach().isfinite().all():\n",
    "#         print(\"enc blew up\")\n",
    "#     if not sae.dec.weight.detach().isfinite().all():\n",
    "#         print(\"dec blew up\")\n",
    "#     d = sae(x)\n",
    "#     l0, reconstruction = d['l0'].detach(), d['reconstruction'].detach()\n",
    "#     if not l0.isfinite().all():\n",
    "#         print(\"l0 blew up\")\n",
    "#     if not reconstruction.isfinite().all():\n",
    "#         print(\"reconstruction blew up\")\n",
    "#     collection.append((l0, reconstruction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_schedule_with_warmup(\n",
    "    current_step: int,\n",
    "    warmup_steps: int,\n",
    "    total_steps: int\n",
    "    ):\n",
    "    if current_step < warmup_steps:\n",
    "        lr =  (1 + current_step) / warmup_steps\n",
    "        # print(lr)\n",
    "        return lr\n",
    "    progress = (current_step - warmup_steps) / (total_steps - warmup_steps)\n",
    "    lr =  0.5 * (1 + math.cos(math.pi * progress))\n",
    "    # print(lr)\n",
    "    return lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparsity_schedule(step, warmup_steps, max_sparsity_coeff):\n",
    "    if step >= warmup_steps:\n",
    "        return max_sparsity_coeff\n",
    "    return step+1 / warmup_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fake_train_loader(batch, d_in, total_steps):\n",
    "    for _ in range(total_steps):\n",
    "        x = torch.randn(batch, d_in)\n",
    "        x = torch.randn(batch, d_in)\n",
    "        x /= vector_norm(x, dim=1, keepdim=True)\n",
    "        yield x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = datasets.load_dataset(\"naraca/mi-dataset-activaciones-llama3_2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['activacion'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = ds['train']\n",
    "train_ds.set_format(type='numpy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:06<00:00, 165.67it/s]\n"
     ]
    }
   ],
   "source": [
    "to_stack = []\n",
    "new_batch_size = 8192//2\n",
    "# we are guaranteed these batches have <= 8192 in ax 0\n",
    "current_batch_size = 0\n",
    "for i, batch in enumerate(tqdm(train_ds['activacion'])):\n",
    "    batch = batch.astype(np.float16)\n",
    "    if current_batch_size + batch.shape[0] < new_batch_size:\n",
    "        to_stack.append(batch)\n",
    "        current_batch_size += batch.shape[0]\n",
    "        continue\n",
    "    to_stack.append(batch[:new_batch_size - current_batch_size])\n",
    "    new_batch = np.concat(to_stack, axis=0)\n",
    "    np.save(f\"/workspace/data/slice{i}.npy\", new_batch)\n",
    "    to_stack.clear()\n",
    "    to_stack.append(batch[new_batch_size - current_batch_size:])\n",
    "    current_batch_size = batch.shape[0] + current_batch_size - new_batch_size\n",
    "\n",
    "    # print(current_batch_size)\n",
    "    # if i > 50:\n",
    "    #     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class NPYFolder(Dataset):\n",
    "    def __init__(self, root):\n",
    "        self.root = pathlib.Path(root)\n",
    "        self.npys = sorted(self.root.glob('*.npy'))\n",
    "    def __len__(self):\n",
    "        return len(self.npys)\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.from_numpy(np.load(self.npys[idx]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_ds = NPYFolder('/workspace/data/')\n",
    "train_dl = DataLoader(train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "163"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.408203125\n",
      "3.2890625\n",
      "3.390625\n",
      "3.388671875\n",
      "3.453125\n",
      "3.44921875\n",
      "3.4453125\n",
      "3.3828125\n",
      "3.375\n",
      "3.455078125\n",
      "3.390625\n",
      "3.458984375\n",
      "3.40625\n",
      "3.42578125\n",
      "3.435546875\n",
      "3.44921875\n",
      "3.47265625\n",
      "3.404296875\n",
      "3.40234375\n",
      "3.4296875\n",
      "3.42578125\n",
      "3.416015625\n",
      "3.400390625\n",
      "3.384765625\n",
      "3.41796875\n",
      "3.3671875\n",
      "3.451171875\n",
      "3.427734375\n",
      "3.435546875\n",
      "3.41796875\n",
      "3.4609375\n",
      "3.43359375\n",
      "3.388671875\n",
      "3.447265625\n",
      "3.396484375\n",
      "3.32421875\n",
      "3.3515625\n",
      "3.41796875\n",
      "3.427734375\n",
      "3.328125\n",
      "3.447265625\n",
      "3.46484375\n",
      "3.4453125\n",
      "3.4453125\n",
      "3.375\n",
      "3.453125\n",
      "3.46484375\n",
      "3.423828125\n",
      "3.40234375\n",
      "3.421875\n",
      "3.361328125\n",
      "3.375\n",
      "3.439453125\n",
      "3.4375\n",
      "3.419921875\n",
      "3.455078125\n",
      "3.392578125\n",
      "3.453125\n",
      "3.455078125\n",
      "3.4140625\n",
      "3.453125\n",
      "3.48828125\n",
      "3.412109375\n",
      "3.466796875\n",
      "3.435546875\n",
      "3.380859375\n",
      "3.3984375\n",
      "3.4140625\n",
      "3.4453125\n",
      "3.44921875\n",
      "3.396484375\n",
      "3.421875\n",
      "3.443359375\n",
      "3.392578125\n",
      "3.42578125\n",
      "3.453125\n",
      "3.421875\n",
      "3.404296875\n",
      "3.443359375\n",
      "3.515625\n",
      "3.400390625\n",
      "3.416015625\n",
      "3.392578125\n",
      "3.40234375\n",
      "3.361328125\n",
      "3.447265625\n",
      "3.45703125\n",
      "3.44921875\n",
      "3.39453125\n",
      "3.380859375\n",
      "3.341796875\n",
      "3.431640625\n",
      "3.443359375\n",
      "3.400390625\n",
      "3.4453125\n",
      "3.42578125\n",
      "3.376953125\n",
      "3.380859375\n",
      "3.390625\n",
      "3.439453125\n",
      "3.4140625\n",
      "3.451171875\n",
      "3.4765625\n",
      "3.38671875\n",
      "3.388671875\n",
      "3.376953125\n",
      "3.451171875\n",
      "3.431640625\n",
      "3.4453125\n",
      "3.470703125\n",
      "3.408203125\n",
      "3.388671875\n",
      "3.48046875\n",
      "3.421875\n",
      "3.462890625\n",
      "3.5078125\n",
      "3.421875\n",
      "3.44140625\n",
      "3.306640625\n",
      "3.16796875\n",
      "3.357421875\n",
      "3.408203125\n",
      "3.4609375\n",
      "3.435546875\n",
      "3.41015625\n",
      "3.44921875\n",
      "3.40234375\n",
      "3.392578125\n",
      "3.408203125\n",
      "3.416015625\n",
      "3.42578125\n",
      "3.470703125\n",
      "3.470703125\n",
      "3.369140625\n",
      "3.431640625\n",
      "3.435546875\n",
      "3.439453125\n",
      "3.431640625\n",
      "3.4140625\n",
      "3.41796875\n",
      "3.419921875\n",
      "3.4453125\n",
      "3.427734375\n",
      "3.421875\n",
      "3.453125\n",
      "3.427734375\n",
      "3.435546875\n",
      "3.435546875\n",
      "3.408203125\n",
      "3.4296875\n",
      "3.4609375\n",
      "3.431640625\n",
      "3.4296875\n",
      "3.400390625\n",
      "3.3984375\n",
      "3.3984375\n",
      "3.42578125\n",
      "3.474609375\n",
      "3.48046875\n",
      "3.443359375\n",
      "3.412109375\n",
      "3.41796875\n",
      "3.41796875\n"
     ]
    }
   ],
   "source": [
    "# mean_norm = \n",
    "for x in train_ds:\n",
    "    norm = torch.linalg.norm(x, dim=1).mean()\n",
    "    print(norm.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 4/163 [00:00<00:10, 15.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reconstruction=13.014205932617188\n",
      "l0=7876.583984375\n",
      "sparsity_coefficient=0.002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 163/163 [00:10<00:00, 14.98it/s]\n",
      "100%|██████████| 163/163 [00:10<00:00, 14.86it/s]\n",
      "100%|██████████| 163/163 [00:10<00:00, 15.55it/s]\n",
      " 10%|█         | 17/163 [00:00<00:06, 21.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reconstruction=1.788827657699585\n",
      "l0=3777.816162109375\n",
      "sparsity_coefficient=6000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 163/163 [00:07<00:00, 20.60it/s]\n",
      "100%|██████████| 163/163 [00:07<00:00, 21.32it/s]\n",
      " 82%|████████▏ | 134/163 [00:06<00:01, 22.31it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 24\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, x \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(tqdm(train_ds)):\n\u001b[1;32m     23\u001b[0m     x \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3.4\u001b[39m\n\u001b[0;32m---> 24\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     26\u001b[0m     d \u001b[38;5;241m=\u001b[39m model(x)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "steps = 2**16\n",
    "max_lr = 7e-5\n",
    "d_in = 2048\n",
    "d_sae = 2048*8\n",
    "model = Sae(d_in, d_sae)\n",
    "model.to('cuda')\n",
    "model.compile()\n",
    "warmup_steps=2000\n",
    "sparcity_warmup_steps=500\n",
    "total_steps=4000 #for now\n",
    "batch = 1024\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=max_lr, betas=(0,0.999))\n",
    "max_sparsity_coeff = 6000\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(\n",
    "    optimizer,\n",
    "    lr_lambda=lambda step: cosine_schedule_with_warmup(step, warmup_steps, total_steps)\n",
    ")\n",
    "# train_ds = fake_train_loader(batch, d_in, total_steps)\n",
    "total_step = 0\n",
    "for epoch in range(100):\n",
    "    for step, x in enumerate(tqdm(train_ds)):\n",
    "        x /= 3.4\n",
    "        x = x.to(dtype).to(\"cuda\")\n",
    "        optimizer.zero_grad()\n",
    "        d = model(x)\n",
    "        reconstruction_loss, l0 = d['reconstruction'], d['l0']\n",
    "        sparsity_coefficient = sparsity_schedule(total_step, sparcity_warmup_steps, max_sparsity_coeff)\n",
    "        loss = reconstruction_loss + sparsity_coefficient * l0\n",
    "        # log losses, compute stats, etc\n",
    "        grad = loss.backward()\n",
    "        # norm = torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        # metrics\n",
    "        if (total_step % 500) == 0:\n",
    "            with torch.no_grad():\n",
    "                # print metrics\n",
    "                print(f\"reconstruction={reconstruction_loss.item()}\")\n",
    "                print(f\"l0={l0.item()}\")\n",
    "                # print(f\"norm={norm.item()}\")\n",
    "                print(f\"{sparsity_coefficient=}\")\n",
    "        optimizer.step()\n",
    "        # TODO: sparsity_coefficient scheduler\n",
    "        # print(scheduler.get_lr())\n",
    "        scheduler.step()\n",
    "\n",
    "        # normalize\n",
    "        with torch.no_grad():\n",
    "            wdecnorm = vector_norm(model.dec.weight, dim=0, keepdim=True)\n",
    "            model.dec.weight /= wdecnorm\n",
    "    # print(f\"epoch loss: {loss.detach().item()}\")\n",
    "        total_step +=1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:0',\n",
       "       dtype=torch.float16, requires_grad=True)"
      ]
     },
     "execution_count": 354,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.log_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.0239, -0.0280, -0.0218,  ...,  0.0366, -0.0213, -0.0363],\n",
       "        [ 0.0276, -0.0075,  0.0171,  ..., -0.0142,  0.0292,  0.0326],\n",
       "        [ 0.0206,  0.0087, -0.0327,  ..., -0.0137, -0.0060, -0.0237],\n",
       "        ...,\n",
       "        [ 0.0017,  0.0329,  0.0052,  ...,  0.0046,  0.0026,  0.0004],\n",
       "        [-0.0197, -0.0227, -0.0090,  ...,  0.0057, -0.0132, -0.0104],\n",
       "        [ 0.0386, -0.0066,  0.0127,  ..., -0.0363,  0.0285,  0.0007]],\n",
       "       device='cuda:0', requires_grad=True)"
      ]
     },
     "execution_count": 312,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.dec.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax. Perhaps you forgot a comma? (1002483432.py, line 13)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[69], line 13\u001b[0;36m\u001b[0m\n\u001b[0;31m    left_over\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax. Perhaps you forgot a comma?\n"
     ]
    }
   ],
   "source": [
    "# new_batch_size = 8192\n",
    "# old_batches_to_stack = []\n",
    "# left_over = np.ndarray([[]], dtype=np.float16)\n",
    "# for i, old_batch in enumerate(tqdm(train_ds['activacion'])):\n",
    "#     # at this point we should have 0 or 1 items in the to_stack list\n",
    "#     # if we have then we take data from it then from old batches\n",
    "#     # construct as much new batches as possible:\n",
    "#     old_batches_sliced = np.array_split(old_batch, new_batch_size)\n",
    "#     for old_batch_slice in old_batches_sliced[:-1]:\n",
    "#         new_batch = np.concatenate([\n",
    "#             left_over,\n",
    "#             old_batch_slice[left_over.shape[0]:]\n",
    "#         ])\n",
    "#         # npy save\n",
    "#         left_over = old_batch_slice[:left_over.shape[0]]\n",
    "#     # check if it's possible to contruct a new batch\n",
    "#     if left_over.shape[0] + old_batches_sliced[-1].shape[0] >= new_batch_size:\n",
    "#         new_batch = np.concatenate([\n",
    "#             left_over,\n",
    "#             old_batches_sliced[-1][left_over.shape[0]:]\n",
    "#             ])\n",
    "#         left_over = old_batches_sliced[-1][:left_over.shape[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:00<00:00, 8006.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved 0\n",
      "saved 1\n",
      "saved 2\n",
      "saved 3\n",
      "saved 4\n",
      "saved 5\n",
      "saved 6\n",
      "saved 7\n",
      "saved 8\n",
      "saved 9\n",
      "saved 11\n",
      "668501\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# new_batch_size = 8192\n",
    "# old_batches_to_stack = []\n",
    "# leftover = None\n",
    "# # invariant at all lines: leftover.shape[0] < new_batch_size\n",
    "# i = 0\n",
    "# total_tokens = 0\n",
    "# for old_batch in tqdm(train_ds['activacion']):\n",
    "#     total_tokens += old_batch.shape[0]\n",
    "#     # construct as much new batches as possible out of the old batch:\n",
    "#     old_batches_sliced = np.array_split(old_batch,\n",
    "#                                         old_batch.shape[0] // new_batch_size\n",
    "#                                         + 1 if old_batch.shape[0] % new_batch_size else 0)\n",
    "#     # print(len(old_batches_sliced))\n",
    "#     # break\n",
    "#     for old_batch_slice in old_batches_sliced:\n",
    "#         if old_batch_slice + 0 if leftover is None else leftover.shape[0] < new_batch_size:\n",
    "#             break\n",
    "#         # all of old_batch_slice is enough to make at least 1 new_batch:\n",
    "#         # collect all its activations\n",
    "#         new_batch = old_batch_slice if leftover is None else np.concatenate([\n",
    "#             leftover,\n",
    "#             old_batch_slice[:leftover.shape[0]]\n",
    "#         ])\n",
    "#         assert new_batch.shape[0] == new_batch_size\n",
    "#         # np.save(f\"/workspace/data/slice{i}.npy\", new_batch)\n",
    "#         print(f\"saved {i}\")\n",
    "#         i += 1\n",
    "#         leftover = old_batch_slice if leftover is None else old_batch_slice[leftover.shape[0]:]\n",
    "#         assert leftover is None or leftover.shape[0] < new_batch_size\n",
    "#     assert old_batches_sliced[-1].shape[0] <= new_batch_size\n",
    "#     # check if it's possible to contruct a new batch\n",
    "#     # notice it would be at most 1 \n",
    "#     if (0 if leftover is None else leftover.shape[0]) + old_batches_sliced[-1].shape[0] >= new_batch_size:\n",
    "#         # print('a', (0 if leftover is None else leftover.shape[0]) + old_batches_sliced[-1].shape[0])\n",
    "#         new_batch = old_batches_sliced[-1] if leftover is None else np.concatenate([\n",
    "#             leftover,\n",
    "#             old_batches_sliced[-1][:new_batch_size - leftover.shape[0]]\n",
    "#             ])\n",
    "#         assert new_batch.shape[0] == new_batch_size\n",
    "#         leftover = old_batches_sliced[-1] if leftover is None else old_batches_sliced[-1][new_batch_size - leftover.shape[0]:]\n",
    "#         assert leftover is None or leftover.shape[0] < new_batch_size\n",
    "#         # np.save(f\"/workspace/data/slice{i}.npy\", new_batch)\n",
    "#         print(f\"saved {i}\")\n",
    "#         i += 1\n",
    "#     else:\n",
    "#         leftover = old_batches_sliced[-1]\n",
    "\n",
    "#     # if leftover is not None:\n",
    "#     #     print(leftover.shape[0])\n",
    "#     # else:\n",
    "#     #     print(0)\n",
    "#     if i >= 20:\n",
    "#         print('exiting')\n",
    "#         break\n",
    "\n",
    "# if leftover is not None:\n",
    "#     # np.save(f\"/workspace/data/slice{i}.npy\", leftover)\n",
    "#     i += 1\n",
    "#     print(f\"saved {i}\")\n",
    "\n",
    "\n",
    "# print(total_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8192"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(old_batches_sliced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = train_ds.select(range(2))\n",
    "a.set_format(type='torch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = a['activacion'][1]\n",
    "t = t.half().numpy()\n",
    "np.save(\"/workspace/data/t.npy\", t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0185, -0.1119,  0.0157,  ..., -0.1203, -0.0603,  0.1016],\n",
       "        [-0.0088, -0.0302, -0.1082,  ..., -0.0330,  0.0157, -0.0439],\n",
       "        [ 0.0186,  0.0679, -0.0203,  ...,  0.0202,  0.1085,  0.1808],\n",
       "        ...,\n",
       "        [ 0.1361, -0.0654,  0.0571,  ...,  0.0568, -0.0781,  0.0022],\n",
       "        [-0.0706, -0.1146,  0.0350,  ...,  0.0627,  0.0798,  0.0478],\n",
       "        [ 0.0402, -0.0783,  0.0140,  ..., -0.0611,  0.1315,  0.0892]],\n",
       "       dtype=torch.float16)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2 = torch.from_numpy(np.load(\"/workspace/data/t.npy\"))\n",
    "t2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "main",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
