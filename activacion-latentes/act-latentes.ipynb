{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrenamiento de un SAE sobre las salidad y activaciones de la MLP intermedia de llama3.2 1B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Costo de entrenamiento\n",
    "\n",
    "Generalmente, el costo computacional de un LLM está dominado por la evaluación\n",
    "de sus MLPs (referencia a el seminario en una universidad de un ex empleado\n",
    "de anthropic)\n",
    "\n",
    "Una aprocimación simple del costo de entrenamiento de un modelo es\n",
    "\n",
    "$$\n",
    "  6ND \n",
    "$$\n",
    "esto en términos de FLOPs (operaciones de punto flotante).\n",
    "(citar a chinchilla scaling laws)\n",
    "\n",
    "donde $N$ es el número de parámetros y $D$ la cantidad de muestras en el\n",
    "conjunto de entrenamiento.\n",
    "\n",
    "Esto se debe a que, generalmente, cada parámetro actua en una multiplicaciónl\n",
    "y en una suma de punto flotante, dandonos un costo de $2ND$ tan solo en el\n",
    "forward pass. Tipicamente, el costo de el backward pass es el doble del forward\n",
    "pass, haciendo que su costo sea $4ND$. Sumando tenemos el resultado previamente\n",
    "mencionado.\n",
    "\n",
    "Sea $N_l$ el número de parámetros de llama.\n",
    "Como nosotros vamos a entrenar un autoencoder sobre un MLP a la mitad de llama,\n",
    "solo necesitamos evaluar esa primera mitad. Además, no correremos el backward\n",
    "pass sobre los parámetros de llama, pues no buscamos modificarlos, es decir, los\n",
    "mantendrémos fijos. Por esto, tenemos que el FLOPs realizados por tal mitad del\n",
    "modelo llama es\n",
    "\n",
    "$$\n",
    "  N_l D\n",
    "$$\n",
    "\n",
    "En cuanto al SAE, solo considerando el costo de aplicar sus matrices, tenemos\n",
    "\n",
    "$$\n",
    "  6 (2d_\\text{in}d_\\text{sae})D\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el caso de gemmascope, entre todos los SAEs que entrenaron, los más pequeños\n",
    "entrenados en las salidas de las MLPs, se entrenaron en 4 mil millones de\n",
    "vectores de activaciones, con la dimencion de los vectores en el stream\n",
    "recidual (y por lo tanto, de la salida de las capas MLP), es 2048, con\n",
    "$d_\\text{sae} = 2028 * 8$.\n",
    "\n",
    "Deseamos encontrar hiperparámetros para entrenar SAEs, para esto:\n",
    "- Usamos una primera aproximación razonable modificando los hiperparámetros para\n",
    "  los autoencoders más pequeños entrenados en salidas de las MLPs de gemma 2\n",
    "  2 B\n",
    "- Ajustamos una power law en base a 2 entrenamientos de SAEs más pequeñas,\n",
    "  usando el mismo learning rate.\n",
    "- Ajustamos una power law para el learning rate con los hiperparámetos optimos\n",
    "  que estimó el paso anterior.\n",
    "\n",
    "Si ignoramos la relación posicional de los tokens y asumimos una distribución\n",
    "uniforme, tenemos que la entropía por token es\n",
    "\n",
    "$$\n",
    "  \\log_2 (\\text{tamaño del vocabulario})\n",
    "$$\n",
    "ya que el vocabulario de gemma2 2B es 256000 y el de llama es 128000, obtenemos\n",
    "que el número de tokens equivalente sería 4.2 mil millones. Ya que nuestro\n",
    "modelo es la mitad de tamaño de gemma2 2B, una primera cantidad de datos\n",
    "razonable para entrenar nuesto sae más grande es $2.1B$\n",
    "\n",
    "En el caso de llama3.2 1B, eso resultaría en\n",
    "\n",
    "$$\n",
    "  N_l D = (2.1 \\times 10^9)(1.2 \\times 10^9) \\approx 2.5 \\times 10^{18}\n",
    "$$\n",
    "Una RTX 4090 puede realisar cada segundo un máximo de $165 \\times 10^{12}$\n",
    "operaciones con tensores de 16 bits y acumulador de 32 bits (referencia\n",
    "al reporte técnico v1.0.1), luego, estimamos 4.2 horas de entrenamiento\n",
    "tan solo considerando la computación en el modelo llama.\n",
    "\n",
    "Ahora, para estimar las horas-RTX4090 para el autoencoder, en el caso de\n",
    "entrenarlo en la salida de la MLP intermedia, tendríamos\n",
    "\n",
    "$$\n",
    "    6(2.1 \\times 10^9)(2048^2)(8)(2) = 8.5 \\times 10^{17}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# El modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\julyv\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (2.6.0)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: jaxtyping in c:\\users\\julyv\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (0.3.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\julyv\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\julyv\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch) (4.13.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\julyv\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\julyv\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\users\\julyv\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch) (2025.3.2)\n",
      "Requirement already satisfied: setuptools in c:\\users\\julyv\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch) (78.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\julyv\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\julyv\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: wadler-lindig>=0.1.3 in c:\\users\\julyv\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from jaxtyping) (0.1.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\julyv\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from jinja2->torch) (3.0.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install torch jaxtyping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m nn\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mjaxtyping\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\julyv\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\__init__.py:274\u001b[0m\n\u001b[0;32m    270\u001b[0m                     \u001b[38;5;28;01mraise\u001b[39;00m err\n\u001b[0;32m    272\u001b[0m         kernel32\u001b[38;5;241m.\u001b[39mSetErrorMode(prev_error_mode)\n\u001b[1;32m--> 274\u001b[0m     \u001b[43m_load_dll_libraries\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    275\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m _load_dll_libraries\n\u001b[0;32m    278\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_preload_cuda_deps\u001b[39m(lib_folder: \u001b[38;5;28mstr\u001b[39m, lib_name: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\julyv\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\__init__.py:250\u001b[0m, in \u001b[0;36m_load_dll_libraries\u001b[1;34m()\u001b[0m\n\u001b[0;32m    248\u001b[0m is_loaded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m with_load_library_flags:\n\u001b[1;32m--> 250\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[43mkernel32\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLoadLibraryExW\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdll\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0x00001100\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    251\u001b[0m     last_error \u001b[38;5;241m=\u001b[39m ctypes\u001b[38;5;241m.\u001b[39mget_last_error()\n\u001b[0;32m    252\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m res \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m last_error \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m126\u001b[39m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import jaxtyping\n",
    "import dataclasses\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JumpReLU(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, threshold):\n",
    "        ctx.save_for_backward(x, threshold)\n",
    "        return torch.where(x > threshold, x, 0)\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        bandwidth = 0.001\n",
    "        x, threshold = ctx.saved_tensors\n",
    "        grad_x = torch.where(x > threshold,\n",
    "            torch.ones_like(x), torch.zeros_like(x))\n",
    "        grad_threshold = torch.where(\n",
    "            abs(x - threshold) < bandwidth/2,\n",
    "            - threshold/bandwidth, 0)\n",
    "\n",
    "        return grad_x * grad_output, grad_threshold * grad_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Step(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, threshold):\n",
    "        ctx.save_for_backward(x, threshold)\n",
    "        return (x > threshold).to(x.dtype)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        bandwidth = 0.001\n",
    "        x, threshold = ctx.saved_tensors\n",
    "\n",
    "        grad_threshold = torch.where(\n",
    "            abs(F.relu(x) - threshold) < bandwidth/2,\n",
    "            -1.0/bandwidth, 0)\n",
    "        \n",
    "        return torch.zeros_like(x), grad_threshold * grad_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sae(nn.Module):\n",
    "    def __init__(self, d_in, d_sae, use_pre_enc_bias=True, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.enc = nn.Linear(d_in, d_sae, dtype=torch.float16)\n",
    "        self.dec = nn.Linear(d_sae, d_in, dtype=torch.float16)\n",
    "        with torch.no_grad():\n",
    "            # normalize each of the d_sae dictonary vectors\n",
    "            self.dec.weight /= self.dec.weight.norm(dim=0, keepdim=True)\n",
    "        self.enc.weight = self.dec.weight.clone().t()\n",
    "        self.enc.bias = torch.zeros_like(self.enc.bias)\n",
    "        self.dec.bias = torch.zeros_like(self.dec.bias)\n",
    "        self.log_threshold = nn.Parameter(\n",
    "            torch.log(torch.full((d_sae,), 0.001, dtype=torch.float16)))\n",
    "        self.use_pre_enc_bias = use_pre_enc_bias\n",
    "        def project_out_parallel_grad(dim, tensor):\n",
    "            @torch.no_grad\n",
    "            def hook(grad_in):\n",
    "                # norm along dim=dim of the tensor is assumed to be 1 as we\n",
    "                # are going to normalize it after every grad update\n",
    "                dot = (tensor * grad_in).sum(dim=dim, keepdim=True)\n",
    "                return grad_in - dot * tensor\n",
    "            return hook\n",
    "\n",
    "        self.dec.weight.register_hook(\n",
    "            project_out_parallel_grad(0, self.dec.weight))\n",
    "                \n",
    "\n",
    "    def forward(self,\n",
    "        x,\n",
    "        return_mask=False,\n",
    "        return_reconstruction=False,\n",
    "        return_l0=True,\n",
    "        return_reconstruction_loss=True,\n",
    "    ):\n",
    "        \"We compute this much here so that compile() can do its magic\"\n",
    "        # as per train_gpt2.py on karpathy's llm.c repo, there are performance\n",
    "        # reasons not to return stuff\n",
    "        d = {}\n",
    "        original_input = x\n",
    "        if self.use_pre_enc_bias:\n",
    "            x = x - self.dec.bias\n",
    "        \n",
    "        x = self.enc(x)\n",
    "        threshold = torch.exp(self.log_threshold)\n",
    "        s = Step.apply(x, threshold)\n",
    "        if return_mask:\n",
    "            d['mask'] = s\n",
    "        if return_l0:\n",
    "            d['l0'] = s.sum(-1).mean()\n",
    "        x = x*s\n",
    "        x = self.dec(x)\n",
    "        if return_reconstruction:\n",
    "            d['reconstruction'] = ((x - original_input)**2).sum(-1).mean()\n",
    "\n",
    "        return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_schedule_with_warmup(\n",
    "    current_step: int,\n",
    "    warmup_steps: int,\n",
    "    total_steps: int\n",
    "    ):\n",
    "    return ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = 2**16\n",
    "max_lr = 7e-5\n",
    "model = Sae(2048, 2048*4)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=max_lr)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(\n",
    "    optimizer,\n",
    "    lr_lambda=lambda step: cosine_schedule_with_warmup(step, warmup_steps, total_tr)\n",
    ")\n",
    "for batch in tqdm(train_loader):\n",
    "    optimizer.zero_grad()\n",
    "    d = model(batch)\n",
    "    reconstruction_loss, l0 = d['reconstuction_loss'], d['l0']\n",
    "    loss = reconstruction_loss + sparsity_coefficient * l0\n",
    "    # log losses, compute stats, etc\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    # TODO: sparsity_coefficient scheduler\n",
    "    scheduler.step()\n",
    "\n",
    "    # normalize\n",
    "    with torch.no_grad():\n",
    "        model.dec.weight /= model.dec.weight.norm(dim=0, keepdim=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting python-dotenv\n",
      "  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
      "Downloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
      "Installing collected packages: python-dotenv\n",
      "Successfully installed python-dotenv-1.1.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The script dotenv.exe is installed in 'c:\\Users\\julyv\\AppData\\Local\\Programs\\Python\\Python313\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpretación de latentes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import openai\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "openai.api_key = os.getenv(\"API_KEY\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def fetch_activating_examples_gpt(n) -> list[tuple[int, list[str]]]:\n",
    "    \n",
    "\n",
    "    system_prompt =(\n",
    "        \"You are a helpful assistant that writes natural, fluent, anad varied sentences related to the concept of communication.\"\n",
    "        \"Each sentence must contain exactly one conceptually significant token related to communication, wrapped in doublle angle brackets like this: <<talk>>. \"\n",
    "    )\n",
    "    user_prompt = (\n",
    "        f\"Generate {n} short sentences (10 to 15 words each) about communication. \"\n",
    "        \"In each sentence, mark exactly one key word per sentence — related to communication — by wrapping it with double angle brackets like this: <<talk>>. \"\n",
    "        \"Do not explain or list anything, just return the marked sentences as plain text, one per line.\"\n",
    "    )\n",
    "\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt},\n",
    "        ],\n",
    "        max_tokens=300,\n",
    "        temperature=0.7\n",
    "    )\n",
    "\n",
    "    raw_sentences = response.choices[0].message.content.strip().split(\"\\n\")\n",
    "\n",
    "    parsed = []\n",
    "    for i, sentence in enumerate(raw_sentences):\n",
    "        tokens = sentence.strip().split()\n",
    "        marked_idx = None\n",
    "        for idx, tok in enumerate(tokens):\n",
    "            if tok.startswith(\"<<\") and tok.endswith(\">>\"):\n",
    "                marked_idx = idx\n",
    "                tokens[idx] = f\"<<{tok.strip('<>')}>>\"\n",
    "                break\n",
    "        \n",
    "        if marked_idx is not None:\n",
    "            parsed.append((i,tokens))\n",
    "\n",
    "    return parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_gpt_tokens(messages: list[dict], model: str = \"gpt-4o\") -> int:\n",
    "    try:\n",
    "        enc = tiktoken.encoding_for_model(model)\n",
    "    except KeyError:\n",
    "        enc = tiktoken.get_encoding(\"cl100k_base\")  # fallback\n",
    "    # Reglas de tokenización para ChatML\n",
    "    tokens_per_message = 3  # <im_start>{role}\\n{content}<im_end>\\n\n",
    "    tokens_per_name = 1     # if name is present\n",
    "    num_tokens = 0\n",
    "    for message in messages:\n",
    "        num_tokens += tokens_per_message\n",
    "        for key, value in message.items():\n",
    "            num_tokens += len(enc.encode(value))\n",
    "            if key == \"name\":\n",
    "                num_tokens += tokens_per_name\n",
    "    num_tokens += 3  # for priming <|start|>assistant\n",
    "    return num_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prompt_gpt4o_from_simulated(\n",
    "        examples: list[tuple[int, list[str]]], \n",
    "        use_chain_of_thought=True) -> dict[str, str]:\n",
    "    \"\"\"\n",
    "    Construye un prompt de GPT-4o a partir de ejemplos simulados con tokens activadores marcados.\n",
    "    \"\"\"\n",
    "    formatted = \"\\n\".join(f\"{i+1}. {' '.join(tokens)}\" for i, tokens in examples)\n",
    "\n",
    "    system_prompt = (\n",
    "       \"You are GPT-4o, an advanced AI model analyzing the behavior of a latent neuron in a transformer-based language model.\\n\"\n",
    "        \"You are given a list of activation examples in <examples></examples>. Each example is a sentence where a single token has been marked \"\n",
    "        \"with double angle brackets (<<token>>) to indicate the token that triggered the strongest activation of this latent.\\n\"\n",
    "        \"Your goal is to reverse-engineer what this neuron is detecting, based on the marked tokens across the examples.\\n\"\n",
    "        \"You must generalize a concept that explains why those tokens might consistently activate this neuron.\\n\"\n",
    "        \"The interpretation must be short (≤20 words), free of formatting, and semantically precise. Avoid generic answers or listing multiple unrelated ideas.\"\n",
    "    )\n",
    "\n",
    "    user_prompt = (\n",
    "        \"<examples>\\n\"\n",
    "        f\"{formatted}\\n\"\n",
    "        \"</examples>\\n\\n\"\n",
    "        \"Now, identify what kind of concept or feature causes this latent neuron to activate.\"\n",
    "    )\n",
    "\n",
    "    if use_chain_of_thought:\n",
    "        assistant_prompt = (\n",
    "            \"First, analyze the examples. Group them if useful. Consider the roles, context, and semantics of the marked tokens.\\n\"\n",
    "            \"Then, provide a clear and concise interpretation — in 20 words or fewer — describing what this neuron detects.\\n\"\n",
    "            \"Final output starts with: This neuron activates on\"\n",
    "    )\n",
    "    else: \n",
    "        assistant_prompt = \"This neuron activates on\"\n",
    "\n",
    "    return {\n",
    "        \"system\": system_prompt,\n",
    "        \"user\": f\"The activating examples are:\\n\\n{formatted}\",\n",
    "        \"assistant\": assistant_prompt,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_batched_prompt_with_token_estimate(\n",
    "    examples: list[tuple[int, list[str]]], \n",
    "    batch_size: int = 10,\n",
    "    use_chain_of_thought: bool = True,\n",
    "    model: str = \"gpt-4o\",\n",
    "    token_limit: int = 500\n",
    ") -> list[dict]:\n",
    "    \"\"\"\n",
    "    Divide los ejemplos en lotes y crea prompts con estimación de tokens para cada lote.\n",
    "    \n",
    "    Args:\n",
    "        examples: Lista de tuplas (índice, tokens) con ejemplos marcados\n",
    "        batch_size: Número de ejemplos por lote\n",
    "        use_chain_of_thought: Si usar cadena de pensamiento en el prompt\n",
    "        model: Modelo para estimación de tokens\n",
    "        token_limit: Límite de tokens para marcar advertencia\n",
    "    \n",
    "    Returns:\n",
    "        Lista de diccionarios con información de cada lote\n",
    "    \"\"\"\n",
    "    batched_prompts = []\n",
    "    \n",
    "    # Dividir ejemplos en lotes\n",
    "    for i in range(0, len(examples), batch_size):\n",
    "        batch_examples = examples[i:i + batch_size]\n",
    "        \n",
    "        # Crear prompt para este lote\n",
    "        prompt = create_prompt_gpt4o_from_simulated(\n",
    "            batch_examples, \n",
    "            use_chain_of_thought=use_chain_of_thought\n",
    "        )\n",
    "        \n",
    "        # Convertir prompt a formato de mensajes para contar tokens\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": prompt[\"system\"]},\n",
    "            {\"role\": \"user\", \"content\": prompt[\"user\"]},\n",
    "            {\"role\": \"assistant\", \"content\": prompt[\"assistant\"]},\n",
    "        ]\n",
    "        \n",
    "        # Contar tokens\n",
    "        n_tokens = count_gpt_tokens(messages, model=model)\n",
    "        \n",
    "        # Crear entrada del lote\n",
    "        batch_info = {\n",
    "            \"prompt\": prompt,\n",
    "            \"examples\": batch_examples,\n",
    "            \"n_tokens\": n_tokens,\n",
    "            \"over_limit\": n_tokens > token_limit,\n",
    "            \"batch_size\": len(batch_examples)\n",
    "        }\n",
    "        \n",
    "        batched_prompts.append(batch_info)\n",
    "    \n",
    "    return batched_prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "APIRemovedInV1",
     "evalue": "\n\nYou tried to access openai.ChatCompletion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n\nYou can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n\nAlternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n\nA detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAPIRemovedInV1\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m simulated_examples \u001b[38;5;241m=\u001b[39m \u001b[43mfetch_activating_examples_gpt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m batched_prompts \u001b[38;5;241m=\u001b[39m build_batched_prompt_with_token_estimate(\n\u001b[0;32m      3\u001b[0m     simulated_examples,\n\u001b[0;32m      4\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m,  \u001b[38;5;66;03m# Puedes ajustar el tamaño\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      7\u001b[0m     token_limit\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m500\u001b[39m  \u001b[38;5;66;03m# Umbral opcional para advertir\u001b[39;00m\n\u001b[0;32m      8\u001b[0m )\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Mostrar estadísticas de cada lote\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[26], line 14\u001b[0m, in \u001b[0;36mfetch_activating_examples_gpt\u001b[1;34m(n)\u001b[0m\n\u001b[0;32m      4\u001b[0m system_prompt \u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are a helpful assistant that writes natural, fluent, anad varied sentences related to the concept of communication.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEach sentence must contain exactly one conceptually significant token related to communication, wrapped in doublle angle brackets like this: <<talk>>. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      7\u001b[0m )\n\u001b[0;32m      8\u001b[0m user_prompt \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenerate \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m short sentences (10 to 15 words each) about communication. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn each sentence, mark exactly one key word per sentence — related to communication — by wrapping it with double angle brackets like this: <<talk>>. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDo not explain or list anything, just return the marked sentences as plain text, one per line.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     12\u001b[0m )\n\u001b[1;32m---> 14\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mopenai\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mChatCompletion\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgpt-4o\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msystem\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msystem_prompt\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser_prompt\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m300\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.7\u001b[39;49m\n\u001b[0;32m     22\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m raw_sentences \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mcontent\u001b[38;5;241m.\u001b[39mstrip()\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     26\u001b[0m parsed \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\Users\\julyv\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\openai\\lib\\_old_api.py:39\u001b[0m, in \u001b[0;36mAPIRemovedInV1Proxy.__call__\u001b[1;34m(self, *_args, **_kwargs)\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m_args: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m_kwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m---> 39\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m APIRemovedInV1(symbol\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_symbol)\n",
      "\u001b[1;31mAPIRemovedInV1\u001b[0m: \n\nYou tried to access openai.ChatCompletion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n\nYou can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n\nAlternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n\nA detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n"
     ]
    }
   ],
   "source": [
    "simulated_examples = fetch_activating_examples_gpt(n=30)\n",
    "batched_prompts = build_batched_prompt_with_token_estimate(\n",
    "    simulated_examples,\n",
    "    batch_size=10,  # Puedes ajustar el tamaño\n",
    "    use_chain_of_thought=True,\n",
    "    model=\"gpt-4o\",\n",
    "    token_limit=500  # Umbral opcional para advertir\n",
    ")\n",
    "\n",
    "# Mostrar estadísticas de cada lote\n",
    "for i, batch in enumerate(batched_prompts):\n",
    "    print(f\"Lote {i+1}: {batch['n_tokens']} tokens. ¿Sobrepasa el límite? {batch['over_limit']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def count_gpt_tokens(messages: list[dict], model: str = \"gpt-4o\") -> int:\n",
    "    try:\n",
    "        enc = tiktoken.encoding_for_model(model)\n",
    "    except KeyError:\n",
    "        enc = tiktoken.get_encoding(\"cl100k_base\")  # fallback\n",
    "\n",
    "    # Reglas de tokenización para ChatML\n",
    "    tokens_per_message = 3  # <im_start>{role}\\n{content}<im_end>\\n\n",
    "    tokens_per_name = 1     # if name is present\n",
    "    num_tokens = 0\n",
    "    for message in messages:\n",
    "        num_tokens += tokens_per_message\n",
    "        for key, value in message.items():\n",
    "            num_tokens += len(enc.encode(value))\n",
    "            if key == \"name\":\n",
    "                num_tokens += tokens_per_name\n",
    "    num_tokens += 3  # for priming <|start|>assistant\n",
    "    return num_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "def get_gpt4o_explanation_from_prompt(prompts: dict, n_completions=5, max_tokens=100, verbose = True) -> list[str]:\n",
    "    \"\"\"\n",
    "    Llama a la API de GPT-4o con un prompt ya generado y devuelve las interpretaciones.\n",
    "    \"\"\"\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": prompts[\"system\"]},\n",
    "        {\"role\": \"user\", \"content\": prompts[\"user\"]},\n",
    "        {\"role\": \"assistant\", \"content\": prompts[\"assistant\"]},\n",
    "    ]\n",
    "\n",
    "    prompt_tokens = count_gpt_tokens(messages, model=\"gpt-4o\")\n",
    "    if verbose:\n",
    "        print(f\"Prompt length: {prompt_tokens} tokens\")\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": prompts[\"system\"]},\n",
    "            {\"role\": \"user\", \"content\": prompts[\"user\"]},\n",
    "            {\"role\": \"assistant\", \"content\": prompts[\"assistant\"]},\n",
    "        ],\n",
    "        n=n_completions,\n",
    "        max_tokens=max_tokens,\n",
    "        temperature=0.7,\n",
    "    )\n",
    "    return [choice.message.content.strip() for choice in response.choices]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Lote 1 ---\n",
      "Prompt length: 395 tokens\n"
     ]
    },
    {
     "ename": "APIRemovedInV1",
     "evalue": "\n\nYou tried to access openai.ChatCompletion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n\nYou can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n\nAlternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n\nA detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAPIRemovedInV1\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[35]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(batched_prompts):\n\u001b[32m      2\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m--- Lote \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m ---\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     respuestas = \u001b[43mget_gpt4o_explanation_from_prompt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprompt\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_completions\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m respuestas:\n\u001b[32m      5\u001b[39m         \u001b[38;5;28mprint\u001b[39m(r)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[34]\u001b[39m\u001b[32m, line 16\u001b[39m, in \u001b[36mget_gpt4o_explanation_from_prompt\u001b[39m\u001b[34m(prompts, n_completions, max_tokens, verbose)\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m verbose:\n\u001b[32m     15\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mPrompt length: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprompt_tokens\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m tokens\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m response = \u001b[43mopenai\u001b[49m\u001b[43m.\u001b[49m\u001b[43mChatCompletion\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgpt-4o\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrole\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msystem\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcontent\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msystem\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrole\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcontent\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrole\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43massistant\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcontent\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43massistant\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_completions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.7\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m [choice.message.content.strip() \u001b[38;5;28;01mfor\u001b[39;00m choice \u001b[38;5;129;01min\u001b[39;00m response.choices]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\openai\\lib\\_old_api.py:39\u001b[39m, in \u001b[36mAPIRemovedInV1Proxy.__call__\u001b[39m\u001b[34m(self, *_args, **_kwargs)\u001b[39m\n\u001b[32m     38\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, *_args: Any, **_kwargs: Any) -> Any:\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m APIRemovedInV1(symbol=\u001b[38;5;28mself\u001b[39m._symbol)\n",
      "\u001b[31mAPIRemovedInV1\u001b[39m: \n\nYou tried to access openai.ChatCompletion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n\nYou can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n\nAlternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n\nA detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n"
     ]
    }
   ],
   "source": [
    "for i, batch in enumerate(batched_prompts):\n",
    "    print(f\"\\n--- Lote {i+1} ---\")\n",
    "    respuestas = get_gpt4o_explanation_from_prompt(batch[\"prompt\"], n_completions=2)\n",
    "    for r in respuestas:\n",
    "        print(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "enviar los datos en grupo batching para obtener descuento en el uso de la API"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
