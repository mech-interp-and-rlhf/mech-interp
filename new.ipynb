{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Conceptos fundamentales\n",
    "\n",
    "**Modelo causal de lenguaje:**  \n",
    "Un modelo como Llama 3.2-1B predice el siguiente token dado un contexto anterior. Está formado por múltiples capas, cada una con un bloque de atención seguido de un MLP (una red lineal‑no‑lineal‑lineal).\n",
    "\n",
    "**Capa MLP-8:**  \n",
    "Corresponde a la novena capa del modelo (índice 8 porque se cuenta desde cero). El objetivo es capturar la salida de esta capa para cada token procesado.\n",
    "\n",
    "**Token:**  \n",
    "Es un número entero que representa una palabra, subpalabra o carácter en el vocabulario del modelo. Un texto se tokeniza en una secuencia de estos enteros.\n",
    "\n",
    "**Token especial:**  \n",
    "Incluyen `<bos>`, `<eos>`, `<pad>`, etc. Son usados para marcar inicios, finales o estructuras sintácticas. Se eliminan del análisis para evitar sesgos.\n",
    "\n",
    "**Activación:**  \n",
    "Vector numérico que representa cómo el modelo codifica la información para un token dado. La salida de la capa MLP-8 tiene dimensión 2048.\n",
    "\n",
    "**Hook:**  \n",
    "Función que intercepta la salida de una parte del modelo sin alterar su comportamiento. Se registra sobre `mlp` de la capa 8 para capturar su activación.\n",
    "\n",
    "### 2. Preparación del entorno\n",
    "\n",
    "El script realiza configuraciones esenciales:\n",
    "\n",
    "- Establece la variable `HF_TOKEN` para autenticarse en Hugging Face.\n",
    "- Define el modelo y dispositivo (`cuda`, `cpu` o `mps`).\n",
    "- Ajusta el tipo de dato (`float16`, `bfloat16` o `float32`) según soporte del sistema.\n",
    "- Carga el tokenizador y modelo desde Hugging Face.\n",
    "\n",
    "```python\n",
    "tok = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=dtype if DEVICE == \"cuda\" else torch.float32,\n",
    "    device_map=\"auto\" if DEVICE == \"cuda\" else None,\n",
    ").eval()\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "### 3. Registro del hook\n",
    "\n",
    "```python\n",
    "mlp_out = None\n",
    "def hook(_m, _i, o):\n",
    "    global mlp_out\n",
    "    mlp_out = o.detach()\n",
    "model.model.layers[8].mlp.register_forward_hook(hook)\n",
    "```\n",
    "\n",
    "Este código asegura que, al pasar datos por el modelo, las activaciones de la capa MLP‑8 se guarden automáticamente en `mlp_out`.\n",
    "\n",
    "\n",
    "\n",
    "### 4. Preprocesamiento de texto y segmentación\n",
    "\n",
    "Se carga el corpus `pile-uncopyrighted` en modo streaming, lo que permite procesar ejemplos sin saturar la memoria.\n",
    "\n",
    "#### Filtrado de calidad\n",
    "\n",
    "```python\n",
    "def good(t): t = t.strip(); return 20 < len(t) < 30_000 and t.upper() != t and not t.isnumeric()\n",
    "```\n",
    "\n",
    "Esta función descarta textos vacíos, completamente en mayúsculas o puramente numéricos.\n",
    "\n",
    "#### División en bloques\n",
    "\n",
    "```python\n",
    "def chunker(text, did):\n",
    "    ids = tok(text, add_special_tokens=False, truncation=True, max_length=TRUNC_TOKS).input_ids\n",
    "    seq = [tok.bos_token_id] + ids + [tok.eos_token_id]\n",
    "    for s in range(0, len(seq) - SEQ_LEN + 1, SEQ_LEN):\n",
    "        yield seq[s:s+SEQ_LEN], did, s\n",
    "```\n",
    "Cada documento válido se divide en segmentos de longitud fija (4096 tokens), respetando los límites del modelo.\n",
    "\n",
    "\n",
    "### 5. Paso por el modelo y filtrado\n",
    "\n",
    "El modelo se evalúa en lotes (`BATCH_FWD = 16`), y se captura la activación `mlp_out`. Luego:\n",
    "\n",
    "- Se filtran los tokens especiales.\n",
    "- Se convierten las activaciones a `torch.uint16` tras proyectarlas desde `bfloat16`.\n",
    "- Se construye un diccionario con metadatos (`doc_id`, `tok_pos`, `token_id`) y la activación comprimida.\n",
    "\n",
    "```python\n",
    "acts_k = acts[i][mask[i]].to(dtype).view(torch.uint16).cpu()\n",
    "```\n",
    "\n",
    "### 6. Construcción del buffer y cálculo parcial de RMS\n",
    "\n",
    "Por cada token válido, se añade una entrada al buffer:\n",
    "\n",
    "```python\n",
    "{\n",
    "  \"doc_id\": doc_buf[i],\n",
    "  \"tok_pos\": pos_buf[i] + j,\n",
    "  \"token_id\": int(tid),\n",
    "  \"activacion\": bits.numpy()\n",
    "}\n",
    "```\n",
    "\n",
    "Simultáneamente, se actualizan acumuladores para el cálculo de la raíz cuadrada media (RMS):\n",
    "\n",
    "```python\n",
    "sum_sq += acts_k.to(torch.float32).pow(2).sum()\n",
    "n_tok  += len(acts_k)\n",
    "```\n",
    "\n",
    "### 7. Shardeo y subida a Hugging Face\n",
    "\n",
    "Cuando el buffer alcanza `SHARD_SIZE = 50_000` ejemplos:\n",
    "\n",
    "- Se crea un shard con `Dataset.from_list(...)`.\n",
    "- Se sube al repositorio Hugging Face con `.push_to_hub(...)`.\n",
    "- Si falla, se guarda localmente en `/workspace/fallback_...`.\n",
    "\n",
    "```python\n",
    "Dataset.from_list(push_buf, features=features).push_to_hub(REPO_ID, split=split, ...)\n",
    "```\n",
    "\n",
    "### 8. Procesamiento final y cómputo del RMS\n",
    "\n",
    "Una vez que se han procesado todos los documentos:\n",
    "\n",
    "- Se calcula el RMS global como:\n",
    "\n",
    "```python\n",
    "rms = math.sqrt(sum_sq / n_tok)\n",
    "```\n",
    "\n",
    "- Se guarda en un archivo `norm_stats.json`.\n",
    "- Se sube al repositorio con `HfApi().upload_file(...)`.\n",
    "\n",
    "Esto proporciona una métrica de escala útil para normalizar las activaciones más adelante.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Definición de un shard\n",
    "\n",
    "Un **shard** es una porción independiente del dataset total, que contiene hasta `SHARD_SIZE = 50_000` ejemplos. Cada ejemplo representa un token válido y sus metadatos:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"doc_id\": 123,\n",
    "  \"tok_pos\": 2048,\n",
    "  \"token_id\": 4871,\n",
    "  \"activacion\": [52839, 39284, 15473, ..., 18924]  // vector codificado en uint16\n",
    "}\n",
    "```\n",
    "\n",
    "La activación se almacena comprimida en `uint16`, reinterpretando los `bfloat16` originales sin perder información binaria, para ahorrar espacio.\n",
    "\n",
    "\n",
    "### 10. Subida a Hugging Face\n",
    "\n",
    "Cuando el buffer `push_buf` alcanza `SHARD_SIZE` filas, se ejecuta:\n",
    "\n",
    "```python\n",
    "Dataset.from_list(push_buf, features=features).push_to_hub(\n",
    "    REPO_ID, split=split, private=True, max_shard_size=\"500MB\"\n",
    ")\n",
    "```\n",
    "\n",
    "Este código:\n",
    "\n",
    "- Crea un `Dataset` estructurado con `Features` fijos.\n",
    "- Lo sube al repositorio `naraca/activaciones-llama3-mlp8` en el split nombrado `train_XXXXX`.\n",
    "- Usa shards privados de tamaño máximo 500MB para facilitar el acceso y evitar fragmentación.\n",
    "\n",
    "\n",
    "### 11. Manejo de errores en la subida\n",
    "\n",
    "Si ocurre una excepción durante la subida (por ejemplo, por red lenta o errores del Hub), se ejecuta el bloque alternativo:\n",
    "\n",
    "```python\n",
    "Dataset.from_list(push_buf, features=features).save_to_disk(f\"/workspace/fallback_{split}\")\n",
    "```\n",
    "\n",
    "Esto asegura que **ningún shard se pierda**, incluso si la conexión falla. Puedes posteriormente subir manualmente esos shards de respaldo.\n",
    "\n",
    "\n",
    "### 12. Proceso final y restos\n",
    "\n",
    "Después del bucle principal, es posible que queden datos no procesados en `mini`, `doc_buf` y `push_buf`. Se ejecuta el mismo procedimiento para esos datos restantes:\n",
    "\n",
    "- Se procesan los ejemplos con el modelo.\n",
    "- Se calcula su activación.\n",
    "- Se filtran y empaquetan.\n",
    "- Se suben como el shard final.\n",
    "\n",
    "Esto garantiza que el dataset sea exhaustivo y que no se pierda información útil.\n",
    "\n",
    "\n",
    "### 13. Cálculo del RMS global\n",
    "\n",
    "Para normalizar las activaciones en futuros entrenamientos, se calcula la raíz cuadrada media (RMS):\n",
    "\n",
    "```python\n",
    "rms = math.sqrt(sum_sq / n_tok)\n",
    "```\n",
    "\n",
    "Donde:\n",
    "\n",
    "- `sum_sq` es la suma total de cuadrados de las activaciones válidas.\n",
    "- `n_tok` es el número total de tokens considerados.\n",
    "\n",
    "Este valor se guarda en un archivo `.json` como:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"tokens\": 3201451,\n",
    "  \"rms\": 1.7294\n",
    "}\n",
    "```\n",
    "\n",
    "Y se sube al mismo repositorio como `norm_stats.json`.\n",
    "\n",
    "\n",
    "### 14. Diseño final del dataset\n",
    "\n",
    "El dataset resultante tiene la siguiente estructura:\n",
    "\n",
    "- **Split:** `train_00000`, `train_00001`, ..., uno por shard.\n",
    "- **Campos por fila:**\n",
    "  - `\"doc_id\"`: ID del documento original.\n",
    "  - `\"tok_pos\"`: posición del token dentro del documento.\n",
    "  - `\"token_id\"`: ID del token según el vocabulario del modelo.\n",
    "  - `\"activacion\"`: lista de 2048 valores `uint16` representando el vector `bfloat16`.\n",
    "\n",
    "Esto permite:\n",
    "\n",
    "- Cargar secuencias alineadas con sus activaciones.\n",
    "- Reconstruir texto si es necesario (usando `token_id`).\n",
    "- Entrenar autoencoders o modelos supervisados.\n",
    "\n",
    "\n",
    "### 15. Verificación de calidad\n",
    "\n",
    "Puedes evaluar la calidad de un shard al cargarlo desde Hugging Face con:\n",
    "\n",
    "```python\n",
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"naraca/activaciones-llama3-mlp8\", split=\"train_00000\")\n",
    "print(ds[0])\n",
    "```\n",
    "\n",
    "También puedes verificar la dispersión de activaciones, histograma de tokens o distribución de posiciones para asegurar una cobertura representativa del corpus.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
